From 5e91bac9fb9cca806e48a2a341b88d015d594385 Mon Sep 17 00:00:00 2001
From: cclauss <cclauss@bluewin.ch>
Date: Tue, 3 Oct 2017 17:07:56 +0200
Subject: [PATCH 1/2] autopep8

---
 .../harvest/barcelona_airquality_harvest.py        | 508 +++++++-------
 .../harvest/madrid_air_quality.py                  | 510 +++++++-------
 .../harvest/madrid_air_quality_harvest.py          | 623 +++++++++--------
 .../harvest/malaga_airqualityobserved_import.py    | 432 ++++++------
 .../AirQualityObserved/harvest/ngsi_helper.py      |  31 +-
 .../harvest/santander_federation.py                | 143 ++--
 PointOfInterest/WeatherStation/stations.py         |  74 +-
 PointOfInterest/import_pois_tourspain.py           | 387 ++++++-----
 Weather/WeatherAlarm/meteoalarm.py                 | 285 ++++----
 Weather/WeatherForecast/harvest/aemet.py           | 399 ++++++-----
 Weather/WeatherForecast/harvest/ipma.py            | 222 +++---
 .../harvest/portugal_weather_forecast_harvest.py   | 472 +++++++------
 .../harvest/spain_weather_forecast_harvest.py      | 769 +++++++++++----------
 .../harvest/portugal_weather_observed_harvest.py   | 387 ++++++-----
 .../harvest/spain_weather_observed_harvest.py      | 506 ++++++++------
 .../WeatherObserved/harvest/weather_observed.py    | 121 ++--
 Weather/aemet.py                                   | 395 ++++++-----
 17 files changed, 3286 insertions(+), 2978 deletions(-)

diff --git a/Environment/AirQualityObserved/harvest/barcelona_airquality_harvest.py b/Environment/AirQualityObserved/harvest/barcelona_airquality_harvest.py
index 9b46a2e..f5da15e 100644
--- a/Environment/AirQualityObserved/harvest/barcelona_airquality_harvest.py
+++ b/Environment/AirQualityObserved/harvest/barcelona_airquality_harvest.py
@@ -3,13 +3,13 @@
 
 '''
   Peforms a data harvesting of air quality official data from the city of Barcelona.
-  
+
   Data source is Catalonia's Government (http://dtes.gencat.cat/)
-  
-  Copyright (c) 2016 Telef贸nica Investigaci贸n y Desarrollo S.A.U. 
-  
+
+  Copyright (c) 2016 Telef贸nica Investigaci贸n y Desarrollo S.A.U.
+
   LICENSE: MIT
-  
+
 '''
 
 from __future__ import with_statement
@@ -38,24 +38,24 @@
 barcelona_tz = timezone('CET')
 
 pollutant_descriptions = {
-  'SO2': 'Sulfur Dioxide',
-  'CO': 'Carbon Monoxide',
-  'NO': 'Nitrogen Monoxide',
-  'NO2': 'Nitrogen Dioxide',
-  'PM2.5': 'Particles less than 2.5',
-  'PM10': 'Particles less than 10',
-  'NOx': 'Nitrogen oxides',
-  'O3': 'Ozone',
-  'TOL': 'Toluene',
-  'BEN': 'Benzene',
-  'C6H6': 'Benzene',
-  'EBE': 'Etilbenzene',
-  'MXY': 'Metaxylene',
-  'PXY': 'Paraxylene',
-  'OXY': 'Orthoxylene',
-  'TCH': 'Total Hydrocarbons',
-  'CH4': 'Hydrocarbons (Methane)',
-  'NHMC': 'Non-methane hydrocarbons (Hexane)'
+    'SO2': 'Sulfur Dioxide',
+    'CO': 'Carbon Monoxide',
+    'NO': 'Nitrogen Monoxide',
+    'NO2': 'Nitrogen Dioxide',
+    'PM2.5': 'Particles less than 2.5',
+    'PM10': 'Particles less than 10',
+    'NOx': 'Nitrogen oxides',
+    'O3': 'Ozone',
+    'TOL': 'Toluene',
+    'BEN': 'Benzene',
+    'C6H6': 'Benzene',
+    'EBE': 'Etilbenzene',
+    'MXY': 'Metaxylene',
+    'PXY': 'Paraxylene',
+    'OXY': 'Orthoxylene',
+    'TCH': 'Total Hydrocarbons',
+    'CH4': 'Hydrocarbons (Methane)',
+    'NHMC': 'Non-methane hydrocarbons (Hexane)'
 }
 
 # Station codes of the Barcelona metropolitan area
@@ -65,7 +65,8 @@
                  '08245012', '08263007', '08263001', '08211004', '08221004',
                  '08301004']
 
-# Provides air quality station information (probably in the future we are not going to need this call)
+# Provides air quality station information (probably in the future we are
+# not going to need this call)
 dataset_url = 'http://dtes.gencat.cat/icqa/AppJava/getEstacio.do?codiEOI={}'
 # Provides per hour data for the current day, per station
 dataset_url2 = 'http://dtes.gencat.cat/icqa/AppJava/getDadesDiaries.do?codiEOI={}'
@@ -76,230 +77,265 @@
 
 # Sanitize string to avoid forbidden characters by Orion
 def sanitize(str_in):
-  return re.sub(r"[<(>)\"\'=;]", "", str_in)
+    return re.sub(r"[<(>)\"\'=;]", "", str_in)
 
 
-# Retrieves all the data from the target stations    
+# Retrieves all the data from the target stations
 def get_air_quality_barcelona(target_stations):
-  # Will keep all the data indexed by station code
-  # An array with one element per hour
-  entity_data = {}
-  
-  f = f2 = None
-  for station in target_stations:
-    entity_data[station] = []
-  
-    logger.debug('Going to harvest data coming from : %s', station)
-    
-    service_url1 = dataset_url.format(station)
-    
-    # Request to obtain station data
-    station_req = urllib2.Request(url=service_url1, headers={'Accept': MIME_JSON})
-    try: f = urllib2.urlopen(station_req)
-    except urllib2.URLError as e:
-      logger.error('Error while calling: %s : %s', service_url1, e)
-      if f != None:
+    # Will keep all the data indexed by station code
+    # An array with one element per hour
+    entity_data = {}
+
+    f = f2 = None
+    for station in target_stations:
+        entity_data[station] = []
+
+        logger.debug('Going to harvest data coming from : %s', station)
+
+        service_url1 = dataset_url.format(station)
+
+        # Request to obtain station data
+        station_req = urllib2.Request(
+            url=service_url1, headers={
+                'Accept': MIME_JSON})
+        try:
+            f = urllib2.urlopen(station_req)
+        except urllib2.URLError as e:
+            logger.error('Error while calling: %s : %s', service_url1, e)
+            if f is not None:
+                f.close()
+            continue
+
+        # deal with wrong encoding
+        json_str = f.read().replace("'", '"')
+        data = json.loads(json_str, encoding='ISO-8859-15')
         f.close()
-      continue
-      
-    # deal with wrong encoding
-    json_str = f.read().replace("'",'"')
-    data = json.loads(json_str,encoding='ISO-8859-15')
-    f.close()
-    
-    service_url2 = dataset_url2.format(station)
-    # Request to obtain pollutants data
-    data_req = urllib2.Request(url=service_url2, headers={'Accept': MIME_JSON})
-    try: f2 = urllib2.urlopen(data_req)
-    except urllib2.URLError as e:
-      logger.error('Error while calling: %s : %s', service_url2, e)
-      if f2 != None:
+
+        service_url2 = dataset_url2.format(station)
+        # Request to obtain pollutants data
+        data_req = urllib2.Request(
+            url=service_url2, headers={
+                'Accept': MIME_JSON})
+        try:
+            f2 = urllib2.urlopen(data_req)
+        except urllib2.URLError as e:
+            logger.error('Error while calling: %s : %s', service_url2, e)
+            if f2 is not None:
+                f2.close()
+            continue
+
+        logger.debug("All data from %s retrieved properly", station)
+
+        # deal with wrong encoding
+        json_pollutants_str = f2.read().replace("'", '"')
+        pollutant_data_st = json.loads(
+            json_pollutants_str, encoding='ISO-8859-15')
         f2.close()
-      continue
-    
-    logger.debug("All data from %s retrieved properly", station)
-    
-    # deal with wrong encoding
-    json_pollutants_str = f2.read().replace("'",'"')
-    pollutant_data_st = json.loads(json_pollutants_str,encoding='ISO-8859-15')    
-    f2.close()
-    
-    station_code = data['codiEOI']
-    
-    pollutant_data = pollutant_data_st['contaminants']
-    
-    for pollutant_info in pollutant_data.values():
-      values = pollutant_info['dadesMesuresDiaria']
-      # It comes with units between parenthesis
-      pollutant_name = pollutant_info['abreviatura'].split('(')[0]
-      pollutant_unit = 'GQ'
-      if pollutant_name == 'CO':
-        pollutant_unit = 'GP'
-        
-      counter = 0
-      hour = 0
-      for v in values:
-        if v['valor'] != '':
-          # Last three values are averages and should be discarded
-          if counter >= (len(values) -3):
-            break
-          
-          hour = counter
-          station_data = None
-          if len(entity_data[station_code]) > hour:
-            station_data = entity_data[station_code][hour]
-            
-          if  station_data is None:
-            station_data = build_station(station_code, data)
-            entity_data[station_code].append(station_data)
-            # 'data' in catalan is 'date' 
-            observ_date = datetime.datetime.strptime(pollutant_data_st['data'],'%d/%m/%Y')
-            observ_date = observ_date.replace(hour=hour,minute=0,second=0,microsecond=0)
-          
-            # Include proper timezone info
-            observ_corrected_date = observ_date.replace(tzinfo=barcelona_tz)
-      
-            station_data['dateObserved'] = {
-              'value': observ_corrected_date.isoformat(),
-              'type': 'DateTime'
-            }
-            
-            # Entity id corresponds to the observed date starting period (in local time)
-            station_data['id'] = 'Barcelona-AirQualityObserved' + '-' + station_code + '-' + observ_date.isoformat()
-            
-            # Convenience data for filtering by target hour
-            station_data['hour'] = {
-              'value': str(hour) + ':' + '00'
-            }
-            
-          value = v['valor']
-          
-          measurand_data = [pollutant_name, value, pollutant_unit, pollutant_descriptions[pollutant_name]]
-          station_data['measurand']['value'].append( ','.join(measurand_data))
-          station_data[pollutant_name] = {
-            'value': float(value)
-          }
-          
-        counter += 1
-    
-    if len(entity_data[station]) > 0:
-      logger.debug("Retrieved data for %s at %s (last hour)",station, entity_data[station][-1]['dateObserved']['value'])
-    else: logger.warn('No data found for station: %s', station)
-  
-  # Now persisting data to Orion Context Broker
-  for a_station in entity_data:
-    data_for_station = entity_data[a_station]
-    print(len(data_for_station))
-    if len(data_for_station):
-      last_measurement = data_for_station[-1]
-      last_measurement['id'] = 'Barcelona-AirQualityObserved' + '-' + last_measurement['stationCode']['value'] + '-' + 'latest'
-      
-    post_station_data(a_station, data_for_station)
+
+        station_code = data['codiEOI']
+
+        pollutant_data = pollutant_data_st['contaminants']
+
+        for pollutant_info in pollutant_data.values():
+            values = pollutant_info['dadesMesuresDiaria']
+            # It comes with units between parenthesis
+            pollutant_name = pollutant_info['abreviatura'].split('(')[0]
+            pollutant_unit = 'GQ'
+            if pollutant_name == 'CO':
+                pollutant_unit = 'GP'
+
+            counter = 0
+            hour = 0
+            for v in values:
+                if v['valor'] != '':
+                    # Last three values are averages and should be discarded
+                    if counter >= (len(values) - 3):
+                        break
+
+                    hour = counter
+                    station_data = None
+                    if len(entity_data[station_code]) > hour:
+                        station_data = entity_data[station_code][hour]
+
+                    if station_data is None:
+                        station_data = build_station(station_code, data)
+                        entity_data[station_code].append(station_data)
+                        # 'data' in catalan is 'date'
+                        observ_date = datetime.datetime.strptime(
+                            pollutant_data_st['data'], '%d/%m/%Y')
+                        observ_date = observ_date.replace(
+                            hour=hour, minute=0, second=0, microsecond=0)
+
+                        # Include proper timezone info
+                        observ_corrected_date = observ_date.replace(
+                            tzinfo=barcelona_tz)
+
+                        station_data['dateObserved'] = {
+                            'value': observ_corrected_date.isoformat(),
+                            'type': 'DateTime'
+                        }
+
+                        # Entity id corresponds to the observed date starting
+                        # period (in local time)
+                        station_data['id'] = 'Barcelona-AirQualityObserved' + \
+                            '-' + station_code + '-' + observ_date.isoformat()
+
+                        # Convenience data for filtering by target hour
+                        station_data['hour'] = {
+                            'value': str(hour) + ':' + '00'
+                        }
+
+                    value = v['valor']
+
+                    measurand_data = [
+                        pollutant_name,
+                        value,
+                        pollutant_unit,
+                        pollutant_descriptions[pollutant_name]]
+                    station_data['measurand']['value'].append(
+                        ','.join(measurand_data))
+                    station_data[pollutant_name] = {
+                        'value': float(value)
+                    }
+
+                counter += 1
+
+        if len(entity_data[station]) > 0:
+            logger.debug("Retrieved data for %s at %s (last hour)",
+                         station, entity_data[station][-1]['dateObserved']['value'])
+        else:
+            logger.warn('No data found for station: %s', station)
+
+    # Now persisting data to Orion Context Broker
+    for a_station in entity_data:
+        data_for_station = entity_data[a_station]
+        print(len(data_for_station))
+        if len(data_for_station):
+            last_measurement = data_for_station[-1]
+            last_measurement['id'] = 'Barcelona-AirQualityObserved' + \
+                '-' + last_measurement['stationCode']['value'] + '-' + 'latest'
+
+        post_station_data(a_station, data_for_station)
 
 
 def build_station(station_code, data):
-  station_data = {
-    'type': AIRQUALITY_TYPE_NAME,
-    'stationCode': {
-      'value': station_code
-    },
-    'stationName': {
-      'value': sanitize(data['nom'])
-    },
-    'address': {
-      'value' : {
-        'addressCountry': 'ES',
-        'addressLocality': sanitize(data['municipi']),
-        'streetAddress': sanitize(data['direccioPostal'])
-      },
-      'type': 'PostalAddress'
-    },
-    'location': {
-      'value': {
-        'type': 'Point',
-        'coordinates': [float(data['longitud']),float(data['latitud'])]
-      },
-      'type': 'geo:json',
-    },
-    # Source of the data Generalitat of Catalonia
-    'source': {
-      'value': 'http://dtes.gencat.cat/',
-      'type': 'URL'
-    },
-    # Provider operator TEF
-    'dataProvider': {
-      'value': 'TEF'
-    },
-    'measurand': {
-      'value': [],
-      'type': 'List'
+    station_data = {
+        'type': AIRQUALITY_TYPE_NAME,
+        'stationCode': {
+            'value': station_code
+        },
+        'stationName': {
+            'value': sanitize(data['nom'])
+        },
+        'address': {
+            'value': {
+                'addressCountry': 'ES',
+                'addressLocality': sanitize(data['municipi']),
+                'streetAddress': sanitize(data['direccioPostal'])
+            },
+            'type': 'PostalAddress'
+        },
+        'location': {
+            'value': {
+                'type': 'Point',
+                'coordinates': [float(data['longitud']), float(data['latitud'])]
+            },
+            'type': 'geo:json',
+        },
+        # Source of the data Generalitat of Catalonia
+        'source': {
+            'value': 'http://dtes.gencat.cat/',
+            'type': 'URL'
+        },
+        # Provider operator TEF
+        'dataProvider': {
+            'value': 'TEF'
+        },
+        'measurand': {
+            'value': [],
+            'type': 'List'
+        }
     }
-  }
-  
-  return station_data
-    
-  
+
+    return station_data
+
+
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_station_data(station_code, data):
-  if len(data) == 0:
-    return
-  
-  payload = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  
-  data_as_str = json.dumps(payload)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SPATH
-  }
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-  
-  logger.debug('Going to persist %s to %s - %d', station_code, orion_service, len(data))
-  
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      logger.debug("Entity successfully created: %s", station_code)
-      persisted_entities = persisted_entities + 1
-  except urllib2.URLError as e:
-    global in_error_entities
-    logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-    logger.debug('Data which failed: %s', data_as_str)
-    in_error_entities = in_error_entities + 1  
-    
+    if len(data) == 0:
+        return
+
+    payload = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+
+    data_as_str = json.dumps(payload)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SPATH
+    }
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    logger.debug(
+        'Going to persist %s to %s - %d',
+        station_code,
+        orion_service,
+        len(data))
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            logger.debug("Entity successfully created: %s", station_code)
+            persisted_entities = persisted_entities + 1
+    except urllib2.URLError as e:
+        global in_error_entities
+        logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+        logger.debug('Data which failed: %s', data_as_str)
+        in_error_entities = in_error_entities + 1
+
 
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_barcelona.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('Barcelona')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-    
-    
+    global logger
+
+    LOG_FILENAME = 'harvest_barcelona.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('Barcelona')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####')
-  logger.debug('Number of air quality stations known: %d', len(station_codes))
-  
-  get_air_quality_barcelona(station_codes)
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of entities in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
+    setup_logger()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug(
+        'Number of air quality stations known: %d',
+        len(station_codes))
+
+    get_air_quality_barcelona(station_codes)
 
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug('Number of entities in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Environment/AirQualityObserved/harvest/madrid_air_quality.py b/Environment/AirQualityObserved/harvest/madrid_air_quality.py
index 5398be8..b129c70 100644
--- a/Environment/AirQualityObserved/harvest/madrid_air_quality.py
+++ b/Environment/AirQualityObserved/harvest/madrid_air_quality.py
@@ -5,7 +5,7 @@
 import csv
 import datetime
 import json
-from   flask import Flask, jsonify, request, Response
+from flask import Flask, jsonify, request, Response
 import urllib2
 import StringIO
 
@@ -14,283 +14,293 @@
 import ngsi_helper
 
 try:
-  xrange          # Python 2
+    xrange          # Python 2
 except NameError:
-  xrange = range  # Python 3
+    xrange = range  # Python 3
 
 app = Flask(__name__)
 
 AMBIENT_TYPE_NAME = 'AmbientObserved'
 
 pollutant_dict = {
-  '01': 'SO2',
-  '06': 'CO',
-  '07': 'NO',
-  '08': 'NO2',
-  '09': 'PM2.5',
-  '10': 'PM10',
-  '12': 'NOx',
-  '14': 'O3',
-  '20': 'TOL',
-  '30': 'BEN',
-  '35': 'EBE',
-  '37': 'MXY',
-  '38': 'PXY',
-  '39': 'OXY',
-  '42': 'TCH',
-  '43': 'CH4',
-  '44': 'NHMC'
+    '01': 'SO2',
+    '06': 'CO',
+    '07': 'NO',
+    '08': 'NO2',
+    '09': 'PM2.5',
+    '10': 'PM10',
+    '12': 'NOx',
+    '14': 'O3',
+    '20': 'TOL',
+    '30': 'BEN',
+    '35': 'EBE',
+    '37': 'MXY',
+    '38': 'PXY',
+    '39': 'OXY',
+    '42': 'TCH',
+    '43': 'CH4',
+    '44': 'NHMC'
 }
 
 pollutant_descriptions = {
-  '01': 'Sulfur Dioxide',
-  '06': 'Carbon Monoxide',
-  '07': 'Nitrogen Monoxide',
-  '08': 'Nitrogen Dioxide',
-  '09': 'Particles < 2.5',
-  '10': 'Particles < 10',
-  '12': 'Nitrogen oxides',
-  '14': 'Ozone',
-  '20': 'Toluene',
-  '30': 'Benzene',
-  '35': 'Etilbenzene',
-  '37': 'Metaxylene',
-  '38': 'Paraxylene',
-  '39': 'Orthoxylene',
-  '42': 'Total Hydrocarbons',
-  '43': 'Hydrocarbons (Methane)',
-  '44': 'Non-methane hydrocarbons (Hexane)'
+    '01': 'Sulfur Dioxide',
+    '06': 'Carbon Monoxide',
+    '07': 'Nitrogen Monoxide',
+    '08': 'Nitrogen Dioxide',
+    '09': 'Particles < 2.5',
+    '10': 'Particles < 10',
+    '12': 'Nitrogen oxides',
+    '14': 'Ozone',
+    '20': 'Toluene',
+    '30': 'Benzene',
+    '35': 'Etilbenzene',
+    '37': 'Metaxylene',
+    '38': 'Paraxylene',
+    '39': 'Orthoxylene',
+    '42': 'Total Hydrocarbons',
+    '43': 'Hydrocarbons (Methane)',
+    '44': 'Non-methane hydrocarbons (Hexane)'
 }
 
 other_dict = {
-  '80': 'ultravioletRadiation',
-  '81': 'windSpeed',
-  '82': 'windDirection',
-  '83': 'temperature',
-  '86': 'relativeHumidity',
-  '87': 'barometricPressure',
-  '88': 'solarRadiation',
-  '89': 'precipitation',
-  '92': 'acidRainLevel'
+    '80': 'ultravioletRadiation',
+    '81': 'windSpeed',
+    '82': 'windDirection',
+    '83': 'temperature',
+    '86': 'relativeHumidity',
+    '87': 'barometricPressure',
+    '88': 'solarRadiation',
+    '89': 'precipitation',
+    '92': 'acidRainLevel'
 }
 
 other_descriptions = {
-  '80': 'Ultraviolet Radiation',
-  '81': 'Wind Speed',
-  '82': 'Wind Direction',
-  '83': 'temperature',
-  '86': 'Relative Humidity',
-  '87': 'Barometric Pressure',
-  '88': 'Solar Radiation',
-  '89': 'Precipitation',
-  '92': 'Acid Rain Level'
+    '80': 'Ultraviolet Radiation',
+    '81': 'Wind Speed',
+    '82': 'Wind Direction',
+    '83': 'temperature',
+    '86': 'Relative Humidity',
+    '87': 'Barometric Pressure',
+    '88': 'Solar Radiation',
+    '89': 'Precipitation',
+    '92': 'Acid Rain Level'
 }
 
 dataset_url = 'http://datos.madrid.es/egob/catalogo/212531-7916318-calidad-aire-tiempo-real.txt'
 orion_service = 'http://130.206.83.68:1026/v1/queryContext'
 
+
 @app.route('/v1/queryContext', methods=['POST'])
 def query_context():
-  msg = json.dumps(request.get_json())
-  # A post to Orion is issued in order to get the concerned entities
-  
-  req = urllib2.Request(url=orion_service, data=msg,
-                        headers={
-                          'Content-Type': 'application/json',
-                          'Accept': 'application/json'
-                        })
-  f = urllib2.urlopen(req)
-  orion_response = json.loads(f.read())
-  f.close()
-  
-  elements = ngsi_helper.parse(orion_response)
-  
-  if len(elements) > 0:
-    now = datetime.datetime.now()
-    if now.minute < 30:
-      target_hour = now.hour - 2
+    msg = json.dumps(request.get_json())
+    # A post to Orion is issued in order to get the concerned entities
+
+    req = urllib2.Request(url=orion_service, data=msg,
+                          headers={
+                              'Content-Type': 'application/json',
+                              'Accept': 'application/json'
+                          })
+    f = urllib2.urlopen(req)
+    orion_response = json.loads(f.read())
+    f.close()
+
+    elements = ngsi_helper.parse(orion_response)
+
+    if len(elements) > 0:
+        now = datetime.datetime.now()
+        if now.minute < 30:
+            target_hour = now.hour - 2
+        else:
+            target_hour = now.hour - 1
+
+        if(target_hour < 0):
+            target_hour = 0
+
+        station_list = []
+        for element in elements:
+            station_code = element['id'].split('-')[2]
+            station_list.append(station_code)
+
+        entities = get_air_quality_madrid(station_list, target_hour)
     else:
-      target_hour = now.hour - 1
-      
-    if(target_hour < 0):
-      target_hour = 0
-    
-    station_list = []
-    for element in elements:
-      station_code = element['id'].split('-')[2]
-      station_list.append(station_code)
-    
-    entities = get_air_quality_madrid(station_list, target_hour)
-  else:
-    entities = []
-  
-  return Response(json.dumps(entities, sort_keys=True),
-                  mimetype='application/json')
-
-  
-@app.route('/v2/entities',  methods=['GET'])
+        entities = []
+
+    return Response(json.dumps(entities, sort_keys=True),
+                    mimetype='application/json')
+
+
+@app.route('/v2/entities', methods=['GET'])
 def v2_end_point():
-  entity_type = request.args.get('type')
-  query = request.args.get('q')
-
-  station_code = ''
-  target_hour = -1
-  if query:
-    tokens  = query.split(';')
-  
-    for token in tokens:
-      items = token.split(':')
-      if items[0] == 'stationCode':
-        station_code = items[1].lower()
-      elif items[0] == 'hour':
-        target_hour = int(items[1])
-        
-  if entity_type == AMBIENT_TYPE_NAME:
-    station_codes = None
-    if station_code:
-      station_codes = [station_code]
-      
-    return Response(json.dumps(get_air_quality_madrid(station_codes, target_hour),
-                               sort_keys=True), mimetype='application/json')
-  else:
-    return Response(json.dumps([]), mimetype='application/json')
-
-    
+    entity_type = request.args.get('type')
+    query = request.args.get('q')
+
+    station_code = ''
+    target_hour = -1
+    if query:
+        tokens = query.split(';')
+
+        for token in tokens:
+            items = token.split(':')
+            if items[0] == 'stationCode':
+                station_code = items[1].lower()
+            elif items[0] == 'hour':
+                target_hour = int(items[1])
+
+    if entity_type == AMBIENT_TYPE_NAME:
+        station_codes = None
+        if station_code:
+            station_codes = [station_code]
+
+        return Response(
+            json.dumps(
+                get_air_quality_madrid(
+                    station_codes,
+                    target_hour),
+                sort_keys=True),
+            mimetype='application/json')
+    else:
+        return Response(json.dumps([]), mimetype='application/json')
+
+
 def get_air_quality_madrid(target_stations, target_hour=-1):
-  req = urllib2.Request(url=dataset_url)
-  f = urllib2.urlopen(req)
-  
-  csv_data = f.read()
-  csv_file = StringIO.StringIO(csv_data)
-  reader = csv.reader(csv_file, delimiter=',')
-  
-  stations = { }
-  
-  for row in reader:
-    station_code = str(row[0]) + str(row[1]) + str(row[2])
-    if station_code and target_stations and not station_code in target_stations:      
-      continue
-    
-    station_num = row[2]
-    if not station_dict[station_num]:
-      continue
-    
-    if not station_code in stations:
-      stations[station_code] = []
-    
-    magnitude = row[3]
-        
-    if (not magnitude in pollutant_dict) and (not magnitude in other_dict):
-      continue
-    
-    is_other = None
-    if magnitude in pollutant_dict:
-      property_name = pollutant_dict[magnitude]
-      property_desc = pollutant_descriptions[magnitude]
-      is_other = False
-   
-    if magnitude in other_dict:
-      property_name = other_dict[magnitude]
-      property_desc = other_descriptions[magnitude]
-      is_other = True
-    
-    print('processing')
-    sys.stdout.flush()
-      
-    hour = 0
-    for x in xrange(9, 57, 2):
-      value = row[x]
-      value_control = row[x + 1]
-      if len(stations[station_code]) < hour + 1:
-        station_data = {
-          'type': AMBIENT_TYPE_NAME,
-          'pollutants': {},
-          'stationCode': station_code,
-          'stationName': station_dict[station_num]['name'],
-          'address': {
-            'addressCountry': 'ES',
-            'addressLocality': 'Madrid',
-            'streetAddress': station_dict[station_num]['address']
-          },
-          'location': station_dict[station_num]['location'] or None,
-          'source': 'http://datos.madrid.es',
-          'dateCreated': datetime.datetime.now().isoformat()
-        }
-        valid_from = datetime.datetime(int(row[6]), int(row[7]), int(row[8]), hour)
-        valid_to = (valid_from + datetime.timedelta(hours=1))
+    req = urllib2.Request(url=dataset_url)
+    f = urllib2.urlopen(req)
 
-        station_data['validity'] = {
-          'from': valid_from.isoformat(),
-          'to': valid_to.isoformat()
-        }
-        station_data['id'] = 'Madrid-AmbientObserved-' + station_code + '-' + valid_from.isoformat()
-        
-        stations[station_code].append(station_data);
-        
-      if value_control == 'V':  
-        param_value = float(value)
-          
-        if not is_other:
-          if property_name == 'CO':
-            param_value = param_value * 1000
-            
-          stations[station_code][hour]['pollutants'][property_name] = {
-            'description': property_desc,
-            'concentration': param_value
-          }
-        else:
-          stations[station_code][hour][property_name] = param_value
-      hour += 1
-  
-  # Returning data as an array
-  station_list = []
-  
-  for station in stations:
-    index_from = 0
-    index_to = len(station_data)
-    station_data = stations[station]
-    if target_hour != -1:
-     index_from = target_hour
-     index_to = index_from + 1
-      
-    data_list = station_data[index_from:index_to]
-    
-    for data in data_list:  
-      if data['pollutants'] or 'temperature' in data:
-        station_list.append(data)
-  return station_list
-
-
-station_dict = { }
+    csv_data = f.read()
+    csv_file = StringIO.StringIO(csv_data)
+    reader = csv.reader(csv_file, delimiter=',')
+
+    stations = {}
 
-def read_station_csv():
-  with open('madrid_airquality_stations.csv', 'rU') as csvfile:
-    reader = csv.reader(csvfile, delimiter=',')
-    
-    index = 0
     for row in reader:
-      if index != 0:
-        station_code = row[2]
-        station_name = row[3]
-        station_address = row[4]
-        station_coords = {
-          'type': 'geo:point',
-          'value': row[1] + ',' + row[0] 
-        }
-        
-        station_dict[station_code.zfill(3)] = {
-          'name': station_name,
-          'address': station_address,
-          'location': station_coords
+        station_code = str(row[0]) + str(row[1]) + str(row[2])
+        if station_code and target_stations and station_code not in target_stations:
+            continue
+
+        station_num = row[2]
+        if not station_dict[station_num]:
+            continue
+
+        if station_code not in stations:
+            stations[station_code] = []
+
+        magnitude = row[3]
+
+        if (magnitude not in pollutant_dict) and (magnitude not in other_dict):
+            continue
+
+        is_other = None
+        if magnitude in pollutant_dict:
+            property_name = pollutant_dict[magnitude]
+            property_desc = pollutant_descriptions[magnitude]
+            is_other = False
+
+        if magnitude in other_dict:
+            property_name = other_dict[magnitude]
+            property_desc = other_descriptions[magnitude]
+            is_other = True
+
+        print('processing')
+        sys.stdout.flush()
+
+        hour = 0
+        for x in xrange(9, 57, 2):
+            value = row[x]
+            value_control = row[x + 1]
+            if len(stations[station_code]) < hour + 1:
+                station_data = {
+                    'type': AMBIENT_TYPE_NAME,
+                    'pollutants': {},
+                    'stationCode': station_code,
+                    'stationName': station_dict[station_num]['name'],
+                    'address': {
+                        'addressCountry': 'ES',
+                        'addressLocality': 'Madrid',
+                        'streetAddress': station_dict[station_num]['address']
+                    },
+                    'location': station_dict[station_num]['location'] or None,
+                    'source': 'http://datos.madrid.es',
+                    'dateCreated': datetime.datetime.now().isoformat()
+                }
+                valid_from = datetime.datetime(
+                    int(row[6]), int(row[7]), int(row[8]), hour)
+                valid_to = (valid_from + datetime.timedelta(hours=1))
+
+                station_data['validity'] = {
+                    'from': valid_from.isoformat(),
+                    'to': valid_to.isoformat()
+                }
+                station_data['id'] = 'Madrid-AmbientObserved-' + \
+                    station_code + '-' + valid_from.isoformat()
+
+                stations[station_code].append(station_data)
+
+            if value_control == 'V':
+                param_value = float(value)
+
+                if not is_other:
+                    if property_name == 'CO':
+                        param_value = param_value * 1000
+
+                    stations[station_code][hour]['pollutants'][property_name] = {
+                        'description': property_desc,
+                        'concentration': param_value
+                    }
+                else:
+                    stations[station_code][hour][property_name] = param_value
+            hour += 1
+
+    # Returning data as an array
+    station_list = []
+
+    for station in stations:
+        index_from = 0
+        index_to = len(station_data)
+        station_data = stations[station]
+        if target_hour != -1:
+            index_from = target_hour
+            index_to = index_from + 1
+
+        data_list = station_data[index_from:index_to]
+
+        for data in data_list:
+            if data['pollutants'] or 'temperature' in data:
+                station_list.append(data)
+    return station_list
+
+
+station_dict = {}
+
+
+def read_station_csv():
+    with open('madrid_airquality_stations.csv', 'rU') as csvfile:
+        reader = csv.reader(csvfile, delimiter=',')
+
+        index = 0
+        for row in reader:
+            if index != 0:
+                station_code = row[2]
+                station_name = row[3]
+                station_address = row[4]
+                station_coords = {
+                    'type': 'geo:point',
+                    'value': row[1] + ',' + row[0]
+                }
+
+                station_dict[station_code.zfill(3)] = {
+                    'name': station_name,
+                    'address': station_address,
+                    'location': station_coords
+                }
+            index += 1
+
+        station_dict['099'] = {
+            'name': 'average',
+            'address': None,
+            'location': None
         }
-      index += 1
-     
-    station_dict['099'] = {
-      'name': 'average',
-      'address': None,
-      'location': None
-    }
-  
+
+
 if __name__ == '__main__':
-  read_station_csv()
-  app.run(host='0.0.0.0',port=1029,debug=True)
+    read_station_csv()
+    app.run(host='0.0.0.0', port=1029, debug=True)
diff --git a/Environment/AirQualityObserved/harvest/madrid_air_quality_harvest.py b/Environment/AirQualityObserved/harvest/madrid_air_quality_harvest.py
index a004578..3d6443c 100644
--- a/Environment/AirQualityObserved/harvest/madrid_air_quality_harvest.py
+++ b/Environment/AirQualityObserved/harvest/madrid_air_quality_harvest.py
@@ -17,15 +17,15 @@
 
 
 try:
-  xrange          # Python 2
+    xrange          # Python 2
 except NameError:
-  xrange = range  # Python 3
+    xrange = range  # Python 3
 
 # Entity type
 AMBIENT_TYPE_NAME = 'AirQualityObserved'
 
 # List of known air quality stations
-station_dict = { }
+station_dict = {}
 
 # Orion service that will store the data
 orion_service = 'http://localhost:1030'
@@ -35,67 +35,67 @@
 madrid_tz = timezone('CET')
 
 pollutant_dict = {
-  '01': 'SO2',
-  '06': 'CO',
-  '07': 'NO',
-  '08': 'NO2',
-  '09': 'PM2.5',
-  '10': 'PM10',
-  '12': 'NOx',
-  '14': 'O3',
-  '20': 'TOL',
-  '30': 'BEN',
-  '35': 'EBE',
-  '37': 'MXY',
-  '38': 'PXY',
-  '39': 'OXY',
-  '42': 'TCH',
-  '43': 'CH4',
-  '44': 'NHMC'
+    '01': 'SO2',
+    '06': 'CO',
+    '07': 'NO',
+    '08': 'NO2',
+    '09': 'PM2.5',
+    '10': 'PM10',
+    '12': 'NOx',
+    '14': 'O3',
+    '20': 'TOL',
+    '30': 'BEN',
+    '35': 'EBE',
+    '37': 'MXY',
+    '38': 'PXY',
+    '39': 'OXY',
+    '42': 'TCH',
+    '43': 'CH4',
+    '44': 'NHMC'
 }
 
 pollutant_descriptions = {
-  '01': 'Sulfur Dioxide',
-  '06': 'Carbon Monoxide',
-  '07': 'Nitrogen Monoxide',
-  '08': 'Nitrogen Dioxide',
-  '09': 'Particles lower than 2.5',
-  '10': 'Particles lower than 10',
-  '12': 'Nitrogen oxides',
-  '14': 'Ozone',
-  '20': 'Toluene',
-  '30': 'Benzene',
-  '35': 'Etilbenzene',
-  '37': 'Metaxylene',
-  '38': 'Paraxylene',
-  '39': 'Orthoxylene',
-  '42': 'Total Hydrocarbons',
-  '43': 'Hydrocarbons - Methane',
-  '44': 'Non-methane hydrocarbons - Hexane'
+    '01': 'Sulfur Dioxide',
+    '06': 'Carbon Monoxide',
+    '07': 'Nitrogen Monoxide',
+    '08': 'Nitrogen Dioxide',
+    '09': 'Particles lower than 2.5',
+    '10': 'Particles lower than 10',
+    '12': 'Nitrogen oxides',
+    '14': 'Ozone',
+    '20': 'Toluene',
+    '30': 'Benzene',
+    '35': 'Etilbenzene',
+    '37': 'Metaxylene',
+    '38': 'Paraxylene',
+    '39': 'Orthoxylene',
+    '42': 'Total Hydrocarbons',
+    '43': 'Hydrocarbons - Methane',
+    '44': 'Non-methane hydrocarbons - Hexane'
 }
 
 other_dict = {
-  '80': 'ultravioletRadiation',
-  '81': 'windSpeed',
-  '82': 'windDirection',
-  '83': 'temperature',
-  '86': 'relativeHumidity',
-  '87': 'barometricPressure',
-  '88': 'solarRadiation',
-  '89': 'precipitation',
-  '92': 'acidRainLevel'
+    '80': 'ultravioletRadiation',
+    '81': 'windSpeed',
+    '82': 'windDirection',
+    '83': 'temperature',
+    '86': 'relativeHumidity',
+    '87': 'barometricPressure',
+    '88': 'solarRadiation',
+    '89': 'precipitation',
+    '92': 'acidRainLevel'
 }
 
 other_descriptions = {
-  '80': 'Ultraviolet Radiation',
-  '81': 'Wind Speed',
-  '82': 'Wind Direction',
-  '83': 'temperature',
-  '86': 'Relative Humidity',
-  '87': 'Barometric Pressure',
-  '88': 'Solar Radiation',
-  '89': 'Precipitation',
-  '92': 'Acid Rain Level'
+    '80': 'Ultraviolet Radiation',
+    '81': 'Wind Speed',
+    '82': 'Wind Direction',
+    '83': 'temperature',
+    '86': 'Relative Humidity',
+    '87': 'Barometric Pressure',
+    '88': 'Solar Radiation',
+    '89': 'Precipitation',
+    '92': 'Acid Rain Level'
 }
 
 dataset_url = 'http://datos.madrid.es/egob/catalogo/212531-7916318-calidad-aire-tiempo-real.txt'
@@ -106,270 +106,295 @@
 
 MIME_JSON = 'application/json'
 FIWARE_SERVICE = 'AirQuality'
-FIWARE_SPATH =   '/Spain_Madrid'
+FIWARE_SPATH = '/Spain_Madrid'
 
 # Sanitize string to avoid forbidden characters by Orion
+
+
 def sanitize(str_in):
-  return re.sub(r"[<(>)\"\'=;]", "", str_in)
+    return re.sub(r"[<(>)\"\'=;]", "", str_in)
 
-    
-# Obtains air quality data and harmonizes it, persisting to Orion    
+
+# Obtains air quality data and harmonizes it, persisting to Orion
 def get_air_quality_madrid():
-  req = urllib2.Request(url=dataset_url)
-  with contextlib.closing(urllib2.urlopen(req)) as f:
-    csv_data = f.read()
-    csv_file = StringIO.StringIO(csv_data)
-    reader = csv.reader(csv_file, delimiter=',')
-    
-    # Dictionary with station data indexed by station code
-    # An array per station code containing one element per hour
-    stations = { }
-    
-    for row in reader:
-      station_code = str(row[0]) + str(row[1]) + str(row[2])
-      
-      station_num = row[2]
-      if not station_dict[station_num]:
-        continue
-      
-      if not station_code in stations:
-        stations[station_code] = []
-      
-      magnitude = row[3]
-              
-      if (not magnitude in pollutant_dict) and (not magnitude in other_dict):
-        continue
-      
-      is_other = None
-      if magnitude in pollutant_dict:
-        property_name = pollutant_dict[magnitude]
-        property_desc = pollutant_descriptions[magnitude]
-        is_other = False
-     
-      if magnitude in other_dict:
-        property_name = other_dict[magnitude]
-        property_desc = other_descriptions[magnitude]
-        is_other = True
-        
-      hour = 0
-      
-      for x in xrange(9, 57, 2):
-        value = row[x]
-        value_control = row[x + 1]
-      
-        if value_control == 'V':
-          # A new entity object is created if it does not exist yet
-          if (len(stations[station_code]) < hour + 1):
-            stations[station_code].append(build_station(station_num, station_code, hour, row))
-          elif (not 'id' in stations[station_code][hour]):
-            stations[station_code][hour] = build_station(station_num, station_code, hour, row)
-            
-          param_value = float(value)
-            
-          if not is_other:
-            unit_code = 'GQ'
-            if property_name == 'CO':
-              unit_code = 'GP'
-            
-            measurand_data = [property_name, str(param_value), unit_code, property_desc]
-            stations[station_code][hour]['measurand']['value'].append(','.join(measurand_data))
-          else:
-            if property_name == 'relativeHumidity':
-              param_value = param_value / 100
-            
-          stations[station_code][hour][property_name] = {
-            'value': param_value
-          }
-        else:
-          # ensure there are no holes in the data
-          if (len(stations[station_code]) < hour + 1):
-            stations[station_code].append({})
-          
-        hour += 1
-    
-      print(len(stations[station_code]))
-    
-    # Now persisting data to Orion Context Broker
-    for station in stations:
-      station_data = stations[station]
-      data_array = []
-      for data in station_data:
-        if 'id' in data:
-          data_array.append(data)
-      if len(data_array) > 0:
-        logger.debug("Retrieved data for %s at %s (last hour)", station, data_array[-1]['dateObserved']['value'])
-        # Last measurement is duplicated to have an entity with the latest measurement obtained
-        last_measurement = data_array[-1]
-        last_measurement['id'] = 'Madrid-AirQualityObserved-' + last_measurement['stationCode']['value'] + '-' + 'latest'
-      else: logger.warn('No data retrieved for: %s', station)
-      
-      post_station_data(station, data_array)
-
-#############    
+    req = urllib2.Request(url=dataset_url)
+    with contextlib.closing(urllib2.urlopen(req)) as f:
+        csv_data = f.read()
+        csv_file = StringIO.StringIO(csv_data)
+        reader = csv.reader(csv_file, delimiter=',')
+
+        # Dictionary with station data indexed by station code
+        # An array per station code containing one element per hour
+        stations = {}
+
+        for row in reader:
+            station_code = str(row[0]) + str(row[1]) + str(row[2])
+
+            station_num = row[2]
+            if not station_dict[station_num]:
+                continue
+
+            if station_code not in stations:
+                stations[station_code] = []
+
+            magnitude = row[3]
+
+            if (magnitude not in pollutant_dict) and (
+                    magnitude not in other_dict):
+                continue
+
+            is_other = None
+            if magnitude in pollutant_dict:
+                property_name = pollutant_dict[magnitude]
+                property_desc = pollutant_descriptions[magnitude]
+                is_other = False
+
+            if magnitude in other_dict:
+                property_name = other_dict[magnitude]
+                property_desc = other_descriptions[magnitude]
+                is_other = True
+
+            hour = 0
+
+            for x in xrange(9, 57, 2):
+                value = row[x]
+                value_control = row[x + 1]
+
+                if value_control == 'V':
+                    # A new entity object is created if it does not exist yet
+                    if (len(stations[station_code]) < hour + 1):
+                        stations[station_code].append(build_station(
+                            station_num, station_code, hour, row))
+                    elif ('id' not in stations[station_code][hour]):
+                        stations[station_code][hour] = build_station(
+                            station_num, station_code, hour, row)
+
+                    param_value = float(value)
+
+                    if not is_other:
+                        unit_code = 'GQ'
+                        if property_name == 'CO':
+                            unit_code = 'GP'
+
+                        measurand_data = [
+                            property_name, str(param_value), unit_code, property_desc]
+                        stations[station_code][hour]['measurand']['value'].append(
+                            ','.join(measurand_data))
+                    else:
+                        if property_name == 'relativeHumidity':
+                            param_value = param_value / 100
+
+                    stations[station_code][hour][property_name] = {
+                        'value': param_value
+                    }
+                else:
+                    # ensure there are no holes in the data
+                    if (len(stations[station_code]) < hour + 1):
+                        stations[station_code].append({})
+
+                hour += 1
+
+            print(len(stations[station_code]))
+
+        # Now persisting data to Orion Context Broker
+        for station in stations:
+            station_data = stations[station]
+            data_array = []
+            for data in station_data:
+                if 'id' in data:
+                    data_array.append(data)
+            if len(data_array) > 0:
+                logger.debug("Retrieved data for %s at %s (last hour)",
+                             station, data_array[-1]['dateObserved']['value'])
+                # Last measurement is duplicated to have an entity with the
+                # latest measurement obtained
+                last_measurement = data_array[-1]
+                last_measurement['id'] = 'Madrid-AirQualityObserved-' + \
+                    last_measurement['stationCode']['value'] + '-' + 'latest'
+            else:
+                logger.warn('No data retrieved for: %s', station)
+
+            post_station_data(station, data_array)
+
+#############
 
 
 # Builds a new entity of type AirQualityObserved
 def build_station(station_num, station_code, hour, row):
-  station_data = {
-    'type': AMBIENT_TYPE_NAME,
-    'measurand': {
-      'type': 'List',
-      'value': []
-    },
-    'stationCode': {
-      'value': station_code
-    },
-    'stationName': {
-      'value': sanitize(station_dict[station_num]['name'])
-    },
-    'address': {
-      'type': 'PostalAddress',
-      'value': {
-        'addressCountry': 'ES',
-        'addressLocality': 'Madrid',
-        'streetAddress': sanitize(station_dict[station_num]['address'])
-      }
-    },
-    'location': {
-      'type': 'geo:json',
-      'value': station_dict[station_num]['location']['value'] or None
-    },
-    'source': {
-      'type': 'URL',
-      'value': 'http://datos.madrid.es'
-    },
-    'dataProvider': {
-      'value': 'TEF'
+    station_data = {
+        'type': AMBIENT_TYPE_NAME,
+        'measurand': {
+            'type': 'List',
+            'value': []
+        },
+        'stationCode': {
+            'value': station_code
+        },
+        'stationName': {
+            'value': sanitize(station_dict[station_num]['name'])
+        },
+        'address': {
+            'type': 'PostalAddress',
+            'value': {
+                'addressCountry': 'ES',
+                'addressLocality': 'Madrid',
+                'streetAddress': sanitize(station_dict[station_num]['address'])
+            }
+        },
+        'location': {
+            'type': 'geo:json',
+            'value': station_dict[station_num]['location']['value'] or None
+        },
+        'source': {
+            'type': 'URL',
+            'value': 'http://datos.madrid.es'
+        },
+        'dataProvider': {
+            'value': 'TEF'
+        }
+    }
+
+    valid_from = datetime.datetime(int(row[6]), int(row[7]), int(row[8]), hour)
+    station_data['id'] = 'Madrid-AirQualityObserved-' + \
+        station_code + '-' + valid_from.isoformat()
+    valid_to = (valid_from + datetime.timedelta(hours=1))
+
+    # Adjust timezones
+    valid_from = valid_from.replace(tzinfo=madrid_tz)
+    valid_to = valid_to.replace(tzinfo=madrid_tz)
+
+    station_data['validity'] = {
+        'value': {
+            'from': valid_from.isoformat(),
+            'to': valid_to.isoformat()
+        },
+        'type': 'StructuredValue'
     }
-  }
-  
-  valid_from = datetime.datetime(int(row[6]), int(row[7]), int(row[8]), hour)
-  station_data['id'] = 'Madrid-AirQualityObserved-' + station_code + '-' + valid_from.isoformat()
-  valid_to = (valid_from + datetime.timedelta(hours=1))
-  
-  # Adjust timezones
-  valid_from = valid_from.replace(tzinfo=madrid_tz)
-  valid_to = valid_to.replace(tzinfo=madrid_tz)
-
-  station_data['validity'] = {
-    'value': {
-      'from': valid_from.isoformat(),
-      'to': valid_to.isoformat()  
-    },
-    'type': 'StructuredValue'
-  }
-  
-  station_data['hour'] = {
-    'value': str(hour) + ':' + '00' 
-  }
-  
-  observ_corrected_date = valid_from
-  station_data['dateObserved'] = {
-    'type': 'DateTime',
-    'value': observ_corrected_date.isoformat() 
-  } 
-    
-  return station_data
 
+    station_data['hour'] = {
+        'value': str(hour) + ':' + '00'
+    }
+
+    observ_corrected_date = valid_from
+    station_data['dateObserved'] = {
+        'type': 'DateTime',
+        'value': observ_corrected_date.isoformat()
+    }
+
+    return station_data
 
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_station_data(station_code, data):
-  if len(data) == 0:
-    return
-  
-  payload = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  
-  data_as_str = json.dumps(payload)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SPATH
-  }
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-  
-  logger.debug('Going to persist %s to %s - %d', station_code, orion_service, len(data))
-  
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      logger.debug("Entity successfully created: %s", station_code)
-      persisted_entities = persisted_entities + 1
-  except urllib2.URLError as e:
-    global in_error_entities
-    logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-    logger.debug('Data which failed: %s', data_as_str)
-    in_error_entities = in_error_entities + 1
-    
+    if len(data) == 0:
+        return
+
+    payload = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+
+    data_as_str = json.dumps(payload)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SPATH
+    }
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    logger.debug(
+        'Going to persist %s to %s - %d',
+        station_code,
+        orion_service,
+        len(data))
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            logger.debug("Entity successfully created: %s", station_code)
+            persisted_entities = persisted_entities + 1
+    except urllib2.URLError as e:
+        global in_error_entities
+        logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+        logger.debug('Data which failed: %s', data_as_str)
+        in_error_entities = in_error_entities + 1
+
 
 # Reads station data from CSV file
 def read_station_csv():
-  with contextlib.closing(open('madrid_airquality_stations.csv', 'rU')) as csvfile:
-    reader = csv.reader(csvfile, delimiter=',')
-    
-    index = 0
-    for row in reader:
-      if index != 0:
-        station_code = row[2]
-        station_name = row[3]
-        station_address = row[4]
-        station_coords = {
-          'type': 'geo:json',
-          'value': {
-            'type': 'Point',
-            'coordinates': [float(row[0]), float(row[1])]
-          } 
-        }
-        
-        station_dict[station_code.zfill(3)] = {
-          'name': station_name,
-          'address': station_address,
-          'location': station_coords
+    with contextlib.closing(open('madrid_airquality_stations.csv', 'rU')) as csvfile:
+        reader = csv.reader(csvfile, delimiter=',')
+
+        index = 0
+        for row in reader:
+            if index != 0:
+                station_code = row[2]
+                station_name = row[3]
+                station_address = row[4]
+                station_coords = {
+                    'type': 'geo:json',
+                    'value': {
+                        'type': 'Point',
+                        'coordinates': [float(row[0]), float(row[1])]
+                    }
+                }
+
+                station_dict[station_code.zfill(3)] = {
+                    'name': station_name,
+                    'address': station_address,
+                    'location': station_coords
+                }
+            index += 1
+
+        station_dict['099'] = {
+            'name': 'average',
+            'address': None,
+            'location': None
         }
-      index += 1
-     
-    station_dict['099'] = {
-      'name': 'average',
-      'address': None,
-      'location': None
-    }
 
 
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_madrid.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('Madrid')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-      
+    global logger
+
+    LOG_FILENAME = 'harvest_madrid.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('Madrid')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  read_station_csv()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####')
-  logger.debug('Number of air quality stations known: %d', len(station_dict.keys()))
-  
-  get_air_quality_madrid()
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of entities in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
-
-  
+    setup_logger()
+
+    read_station_csv()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug(
+        'Number of air quality stations known: %d', len(
+            station_dict.keys()))
+
+    get_air_quality_madrid()
+
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug('Number of entities in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Environment/AirQualityObserved/harvest/malaga_airqualityobserved_import.py b/Environment/AirQualityObserved/harvest/malaga_airqualityobserved_import.py
index 996e57f..dbcb036 100644
--- a/Environment/AirQualityObserved/harvest/malaga_airqualityobserved_import.py
+++ b/Environment/AirQualityObserved/harvest/malaga_airqualityobserved_import.py
@@ -14,14 +14,14 @@
 import contextlib
 
 pollutant_descriptions = {
-  'NO2': 'Nitrogen Dioxide',
-  'CO': 'Carbon Monoxide'
+    'NO2': 'Nitrogen Dioxide',
+    'CO': 'Carbon Monoxide'
 }
 
 # Bootstrap data about districts in Malaga
-districts_data = { }
+districts_data = {}
 # Bootstrap data about neighbourhoods in Malaga
-neighbourhoods_data = { }
+neighbourhoods_data = {}
 
 FIWARE_SERVICE = 'airquality'
 FIWARE_SERVICE_PATH = '/Spain_Malaga'
@@ -33,229 +33,239 @@
 in_error_entities = 0
 
 # Entity data is indexed by "zone-zone type" and then by date/time
-airquality_data = { }
+airquality_data = {}
 
 
 def read_geojson_data():
-  with contextlib.closing(open('Distritos.geojson', 'rU')) as data_file:    
-    data = json.load(data_file)
-        
-    features = data['features']
-    for feature in features:
-      properties = feature['properties']
-      num_district = str(properties['NUMERO'])
-      districts_data[num_district] = {
-        'name': properties['NOMBRE'],
-        'geometry': feature['geometry']
-      }
-   
-  with contextlib.closing(open('Barrios.geojson', 'rU')) as data_file:    
-    data = json.load(data_file)
-        
-    features = data['features']
-    for feature in features:
-      properties = feature['properties']
-      num_neighboourhood = str(properties['NUMBARRIO'])
-      neighbourhoods_data[num_neighboourhood] = {
-        'name': properties['NOMBARRIO'],
-        'geometry': feature['geometry']
-      }         
+    with contextlib.closing(open('Distritos.geojson', 'rU')) as data_file:
+        data = json.load(data_file)
+
+        features = data['features']
+        for feature in features:
+            properties = feature['properties']
+            num_district = str(properties['NUMERO'])
+            districts_data[num_district] = {
+                'name': properties['NOMBRE'],
+                'geometry': feature['geometry']
+            }
+
+    with contextlib.closing(open('Barrios.geojson', 'rU')) as data_file:
+        data = json.load(data_file)
+
+        features = data['features']
+        for feature in features:
+            properties = feature['properties']
+            num_neighboourhood = str(properties['NUMBARRIO'])
+            neighbourhoods_data[num_neighboourhood] = {
+                'name': properties['NOMBARRIO'],
+                'geometry': feature['geometry']
+            }
 
 # Processes a CSV entry which corresponds to a certain kind of pollutant
+
+
 def process_csv_row(row, pollutant, unit_code):
-  zone = row[0]
-  zone_type = row[1]
-  
-  date = row[2].replace(' ', 'T')
-  value = row[3]
-  
-  if value == '0':
-    return
-  
-  if zone_type != '1' and zone_type != '2':
-    return
-  
-  if zone_type == '1':
-    # District
-    area_data = districts_data
-  elif zone_type == '2':
-    # Neighbourhood
-    area_data = neighbourhoods_data
-  
-  if zone not in area_data:
-    return
-  
-  geometry = area_data[zone]['geometry']
-  place_name = area_data[zone]['name']
-  
-  key = zone + '-' + zone_type + '-' + date
-  if key not in airquality_data:
-    entity = build_entity(zone, zone_type, date)
-    airquality_data[key] = entity
-  else:
-    entity = airquality_data[key]
-  
-  if entity is None:
-    return
-  
-  # Now adding the pollutant data
-  pollutant_data = pollutant + ', ' + str(float(value)) + ', ' + unit_code + ', ' + pollutant_descriptions[pollutant]
-  entity['measurand']['value'].append(pollutant_data)
-  entity[pollutant] = {
-    'value': float(value)
-  }
-
-  
-# Builds entity data for the zone, zone type and data    
+    zone = row[0]
+    zone_type = row[1]
+
+    date = row[2].replace(' ', 'T')
+    value = row[3]
+
+    if value == '0':
+        return
+
+    if zone_type != '1' and zone_type != '2':
+        return
+
+    if zone_type == '1':
+        # District
+        area_data = districts_data
+    elif zone_type == '2':
+        # Neighbourhood
+        area_data = neighbourhoods_data
+
+    if zone not in area_data:
+        return
+
+    geometry = area_data[zone]['geometry']
+    place_name = area_data[zone]['name']
+
+    key = zone + '-' + zone_type + '-' + date
+    if key not in airquality_data:
+        entity = build_entity(zone, zone_type, date)
+        airquality_data[key] = entity
+    else:
+        entity = airquality_data[key]
+
+    if entity is None:
+        return
+
+    # Now adding the pollutant data
+    pollutant_data = pollutant + ', ' + \
+        str(float(value)) + ', ' + unit_code + ', ' + pollutant_descriptions[pollutant]
+    entity['measurand']['value'].append(pollutant_data)
+    entity[pollutant] = {
+        'value': float(value)
+    }
+
+
+# Builds entity data for the zone, zone type and data
 def build_entity(zone, zone_type, date):
-  if zone_type == '1':
-    # District
-    area_data = districts_data
-  elif zone_type == '2':
-    # Neighbourhood
-    area_data = neighbourhoods_data
-      
-  if zone not in area_data:
-    logger.warn('Zone data not found for: %s', zone)
-    return None
-      
-  geometry = area_data[zone]['geometry']
-  place_name = area_data[zone]['name']
-      
-  entity = {
-    'id': 'Malaga-AirQualityObserved' + '-' + zone_type + '-' + zone + '-' + date,
-    'type': 'AirQualityObserved',
-    'location': {
-      'type': 'geo:json',
-      'value': geometry
-    },
-    'address': {
-      'type': 'PostalAddress',
-      'value': {
-        'addressLocality': 'Malaga',
-        'addressCountry': 'ES',
-        'areaServed': place_name
-      }
-    },
-    'dateObserved': {
-      'type': 'DateTime',
-      'value': date
-    },
-    'measurand': {
-      'type': 'List',
-      'value': [ ]
-    },
-    'source': {
-      'type': 'URL',
-      'value': 'http://www.edpingenieria.com/'
-    },
-  }
-    
-  return entity
+    if zone_type == '1':
+        # District
+        area_data = districts_data
+    elif zone_type == '2':
+        # Neighbourhood
+        area_data = neighbourhoods_data
+
+    if zone not in area_data:
+        logger.warn('Zone data not found for: %s', zone)
+        return None
+
+    geometry = area_data[zone]['geometry']
+    place_name = area_data[zone]['name']
+
+    entity = {
+        'id': 'Malaga-AirQualityObserved' + '-' + zone_type + '-' + zone + '-' + date,
+        'type': 'AirQualityObserved',
+        'location': {
+            'type': 'geo:json',
+            'value': geometry},
+        'address': {
+            'type': 'PostalAddress',
+            'value': {
+                'addressLocality': 'Malaga',
+                'addressCountry': 'ES',
+                'areaServed': place_name}},
+        'dateObserved': {
+            'type': 'DateTime',
+                    'value': date},
+        'measurand': {
+            'type': 'List',
+            'value': []},
+        'source': {
+            'type': 'URL',
+            'value': 'http://www.edpingenieria.com/'},
+    }
+
+    return entity
 
 
 # Reads data from CSV file (origin of data are public buses)
 def get_malaga_airquality():
-  with contextlib.closing(open('hourly_no2.csv', 'rU')) as csvfile:
-    reader = csv.reader(csvfile, delimiter=',')
-    
-    index = 0
-    for row in reader:
-      if index == 0:
-        index += 1
-        continue
-      
-      process_csv_row(row, 'NO2', 'GQ')
-  
-  with contextlib.closing(open('octohourly_co.csv', 'rU')) as csvfile:
-    reader = csv.reader(csvfile, delimiter=',')
-    
-    index = 0
-    for row in reader:
-      if index == 0:
-        index += 1
-        continue
-      
-      process_csv_row(row, 'CO', 'GP')      
-
-  logger.debug('Number of entities built: %d', len(airquality_data.keys()))
-  # Entities are persisted on batches of 10 elements
-  entity_buffer = []
-  for entry in airquality_data:
-    entity_buffer.append(airquality_data[entry])
-    
-    if len(entity_buffer) == 10:
-      post_data(entity_buffer)
-      entity_buffer = []
-  
-  # Last set of entities is persisted
-  post_data(entity_buffer)
+    with contextlib.closing(open('hourly_no2.csv', 'rU')) as csvfile:
+        reader = csv.reader(csvfile, delimiter=',')
+
+        index = 0
+        for row in reader:
+            if index == 0:
+                index += 1
+                continue
+
+            process_csv_row(row, 'NO2', 'GQ')
+
+    with contextlib.closing(open('octohourly_co.csv', 'rU')) as csvfile:
+        reader = csv.reader(csvfile, delimiter=',')
+
+        index = 0
+        for row in reader:
+            if index == 0:
+                index += 1
+                continue
+
+            process_csv_row(row, 'CO', 'GP')
+
+    logger.debug('Number of entities built: %d', len(airquality_data.keys()))
+    # Entities are persisted on batches of 10 elements
+    entity_buffer = []
+    for entry in airquality_data:
+        entity_buffer.append(airquality_data[entry])
+
+        if len(entity_buffer) == 10:
+            post_data(entity_buffer)
+            entity_buffer = []
+
+    # Last set of entities is persisted
+    post_data(entity_buffer)
 
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_data(data):
-  if len(data) == 0:
-    return
-  
-  entity_ids = []
-  for entity in data:
-    entity_ids.append(entity['id'])
-  
-  payload = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  
-  data_as_str = json.dumps(payload)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SERVICE_PATH
-  }
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-    
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      logger.debug("Entity batch successfully created: %s", ', '.join(entity_ids))
-      persisted_entities = persisted_entities + 1
-  except urllib2.URLError as e:
-    global in_error_entities
-    logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-    # logger.debug('Data which failed: %s', data_as_str)
-    in_error_entities = in_error_entities + 1
+    if len(data) == 0:
+        return
+
+    entity_ids = []
+    for entity in data:
+        entity_ids.append(entity['id'])
+
+    payload = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+
+    data_as_str = json.dumps(payload)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SERVICE_PATH
+    }
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            logger.debug(
+                "Entity batch successfully created: %s",
+                ', '.join(entity_ids))
+            persisted_entities = persisted_entities + 1
+    except urllib2.URLError as e:
+        global in_error_entities
+        logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+        # logger.debug('Data which failed: %s', data_as_str)
+        in_error_entities = in_error_entities + 1
 
 
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_airquality_malaga.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('Malaga')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-      
+    global logger
+
+    LOG_FILENAME = 'harvest_airquality_malaga.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('Malaga')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####')
-  
-  read_geojson_data()
-  
-  logger.debug('Number of districts known: %d', len(districts_data.keys()))
-  logger.debug('Number of neighbourhoods known: %d', len(neighbourhoods_data.keys()))
-  
-  get_malaga_airquality()
-  
-  logger.debug('#### Harvesting and harmonization cycle ... finished ####')
+    setup_logger()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+
+    read_geojson_data()
+
+    logger.debug('Number of districts known: %d', len(districts_data.keys()))
+    logger.debug('Number of neighbourhoods known: %d',
+                 len(neighbourhoods_data.keys()))
+
+    get_malaga_airquality()
+
+    logger.debug('#### Harvesting and harmonization cycle ... finished ####')
diff --git a/Environment/AirQualityObserved/harvest/ngsi_helper.py b/Environment/AirQualityObserved/harvest/ngsi_helper.py
index c8f9c89..114dccf 100644
--- a/Environment/AirQualityObserved/harvest/ngsi_helper.py
+++ b/Environment/AirQualityObserved/harvest/ngsi_helper.py
@@ -1,18 +1,19 @@
 # -*- coding: utf-8 -*-
 
+
 def parse(ngsi_obj):
-  out = []
-  if 'contextResponses' in ngsi_obj:
-    responses = ngsi_obj['contextResponses']
-    for response in responses:
-      element = response['contextElement']
-      attributes = element['attributes']
-      out.append({
-        'id': element['id'],
-        'location': {
-          'type': 'geo:point',
-          'value': attributes[0]['value']
-        }
-      })
-      
-  return out
\ No newline at end of file
+    out = []
+    if 'contextResponses' in ngsi_obj:
+        responses = ngsi_obj['contextResponses']
+        for response in responses:
+            element = response['contextElement']
+            attributes = element['attributes']
+            out.append({
+                'id': element['id'],
+                'location': {
+                    'type': 'geo:point',
+                    'value': attributes[0]['value']
+                }
+            })
+
+    return out
diff --git a/Environment/AirQualityObserved/harvest/santander_federation.py b/Environment/AirQualityObserved/harvest/santander_federation.py
index 3d9139c..471cfa2 100644
--- a/Environment/AirQualityObserved/harvest/santander_federation.py
+++ b/Environment/AirQualityObserved/harvest/santander_federation.py
@@ -1,11 +1,12 @@
 # This simple Flask application allows to listen for incoming notification
 # messages with air quality data from servers owned by the city of Santander
-# The notification messages are listened and data is sent to FIWARE GSMA instance
+# The notification messages are listened and data is sent to FIWARE GSMA
+# instance
 
 from __future__ import with_statement
 from __future__ import print_function
 import json
-from  flask import Flask, jsonify, request, Response
+from flask import Flask, jsonify, request, Response
 import urllib2
 import contextlib
 import logging
@@ -21,79 +22,89 @@
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_data(data):
-  app.logger.debug('Going to post data %d', len(data))
-    
-  if len(data) == 0:
-    return
-  
-  payload = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  
-  data_as_str = json.dumps(payload)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SERVICE_PATH
-  }
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-    
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      app.logger.debug("Entity batch successfully created!")
-  except urllib2.URLError as e:
-    app.logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-
-
-def build_logger_handler():  
-  LOG_FILENAME = 'harvest_airquality_santander.log'
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  handler.setLevel(logging.DEBUG)
-  
-  return handler
+    app.logger.debug('Going to post data %d', len(data))
+
+    if len(data) == 0:
+        return
+
+    payload = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+
+    data_as_str = json.dumps(payload)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SERVICE_PATH
+    }
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            app.logger.debug("Entity batch successfully created!")
+    except urllib2.URLError as e:
+        app.logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+
+
+def build_logger_handler():
+    LOG_FILENAME = 'harvest_airquality_santander.log'
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+    handler.setLevel(logging.DEBUG)
+
+    return handler
 
 
 @app.route("/")
 def hello():
-  return "Hello World!"
+    return "Hello World!"
 
 
-@app.route("/federate",methods=['POST'])
+@app.route("/federate", methods=['POST'])
 def process():
-  data = request.get_json()
-  # Check that subscription id is correct
-  if not 'subscriptionId' in data or not 'data' in data:
-    app.logger.warn('JSON payload seems not to be appropriate')
-    return ('', 200)
+    data = request.get_json()
+    # Check that subscription id is correct
+    if 'subscriptionId' in data or not 'data' not in data:
+        app.logger.warn('JSON payload seems not to be appropriate')
+        return ('', 200)
+
+    subscriptionId = data['subscriptionId']
+    if subscriptionId != SUBSCRIPTION_ID:
+        app.logger.warn(
+            'Subscription id : %s. Not recognized!!',
+            subscriptionId)
+        return ('', 200)
+
+    app.logger.debug('Subscription id is correct, posting data')
+    # TODO: Validate the data using the JSON-Schema for air quality
+    post_data(data['data'])
 
-  subscriptionId = data['subscriptionId']
-  if subscriptionId != SUBSCRIPTION_ID:
-    app.logger.warn('Subscription id : %s. Not recognized!!', subscriptionId)
     return ('', 200)
-  
-  app.logger.debug('Subscription id is correct, posting data')
-  # TODO: Validate the data using the JSON-Schema for air quality
-  post_data(data['data'])
-  
-  return ('', 200)
 
 
 if __name__ == "__main__":
-  print('Running')
-  handler = build_logger_handler()
-  app.logger.addHandler(handler)
-  
-  print(handler)
-  
-  app.logger.debug('Starting server ....')
-  
-  app.run(debug=True, host="0.0.0.0", port=int("1050"))
-    
\ No newline at end of file
+    print('Running')
+    handler = build_logger_handler()
+    app.logger.addHandler(handler)
+
+    print(handler)
+
+    app.logger.debug('Starting server ....')
+
+    app.run(debug=True, host="0.0.0.0", port=int("1050"))
diff --git a/PointOfInterest/WeatherStation/stations.py b/PointOfInterest/WeatherStation/stations.py
index 5912f2b..f2c29d8 100755
--- a/PointOfInterest/WeatherStation/stations.py
+++ b/PointOfInterest/WeatherStation/stations.py
@@ -3,41 +3,43 @@
 
 import csv
 
+
 def main():
- csvoutput = open('stations-normalized.csv', 'wb')
- station_writer = csv.writer(csvoutput, delimiter=',')
-  
- with open('stations.csv', 'rU') as csvfile:
-  reader = csv.reader(csvfile, delimiter=';')
-  index = 0
-  for row in reader:
-    if index != 0:
-      latitude = row[5].strip()
-      longitude = row[6].strip()
-      
-      out_latitude = ''
-      out_longitude = ''
-      
-      out_latitude = latitude[0:2]
-      out_latitude += ' '
-      out_latitude += latitude[2:4]
-      out_latitude += ' '
-      out_latitude += latitude[4:6]
-      
-      if longitude[6] == 'W':
-        out_longitude = '-'
-        
-      out_longitude += longitude[0:2]
-      out_longitude += ' '
-      out_longitude += longitude[2:4]
-      out_longitude += ' '
-      out_longitude += longitude[4:6]
-      
-      row[5] = out_latitude
-      row[6] = out_longitude
-     
-    station_writer.writerow(row)
-    index +=1  
-  
+    csvoutput = open('stations-normalized.csv', 'wb')
+    station_writer = csv.writer(csvoutput, delimiter=',')
+
+    with open('stations.csv', 'rU') as csvfile:
+        reader = csv.reader(csvfile, delimiter=';')
+        index = 0
+        for row in reader:
+            if index != 0:
+                latitude = row[5].strip()
+                longitude = row[6].strip()
+
+                out_latitude = ''
+                out_longitude = ''
+
+                out_latitude = latitude[0:2]
+                out_latitude += ' '
+                out_latitude += latitude[2:4]
+                out_latitude += ' '
+                out_latitude += latitude[4:6]
+
+                if longitude[6] == 'W':
+                    out_longitude = '-'
+
+                out_longitude += longitude[0:2]
+                out_longitude += ' '
+                out_longitude += longitude[2:4]
+                out_longitude += ' '
+                out_longitude += longitude[4:6]
+
+                row[5] = out_latitude
+                row[6] = out_longitude
+
+            station_writer.writerow(row)
+            index += 1
+
+
 if __name__ == '__main__':
-  main()
\ No newline at end of file
+    main()
diff --git a/PointOfInterest/import_pois_tourspain.py b/PointOfInterest/import_pois_tourspain.py
index c7481ca..a9485a1 100644
--- a/PointOfInterest/import_pois_tourspain.py
+++ b/PointOfInterest/import_pois_tourspain.py
@@ -12,10 +12,10 @@
 import urllib2
 import contextlib
 
-DEFAULT_SOURCE_FOLDER =       'INECO'
-BEACH_FOLDER =                '20170510_playas/playas'
-MUSEUM_FOLDER =               '20170510_museos/museos'
-TOURIST_INFO_FOLDER =         '20170510_oficinasturismo/oficina-turismo'
+DEFAULT_SOURCE_FOLDER = 'INECO'
+BEACH_FOLDER = '20170510_playas/playas'
+MUSEUM_FOLDER = '20170510_museos/museos'
+TOURIST_INFO_FOLDER = '20170510_oficinasturismo/oficina-turismo'
 
 folders = [BEACH_FOLDER, MUSEUM_FOLDER, TOURIST_INFO_FOLDER]
 
@@ -24,216 +24,231 @@
 cities = ['barcelona', 'coruna_a', 'malaga', 'sevilla', 'valencia']
 
 description_nodes = ['TipoPlaya', 'TipoMuseo', 'TipoOficinaTurismo']
- 
+
 # Orion service that will store the data
 orion_service = 'http://localhost:1030'
 
 FIWARE_SERVICE = 'poi'
-FIWARE_SPATH   = '/Spain'
+FIWARE_SPATH = '/Spain'
 
 MIME_JSON = 'application/json'
 
 # Dictionary containing all the POIs
-pois = { }
+pois = {}
 
 persisted_entities = 0
 in_error_entities = 0
 
 # Sanitize string to avoid forbidden characters by Orion
+
+
 def sanitize(str_in):
-  return re.sub(r"[<(>)\"\'=;\n\r]", "", str_in)
+    return re.sub(r"[<(>)\"\'=;\n\r]", "", str_in)
+
 
 def transform_data(source_folder_param):
-  index_poi_type = 0
-  
-  source_folder = source_folder_param
-  if source_folder is None:
-    source_folder = DEFAULT_SOURCE_FOLDER
-  
-  for folder in folders:
-    poi_list = []
-    pois[categories_names[index_poi_type]] = poi_list
-    
-    for city in cities: 
-      city_folder = os.path.join(source_folder, folder, city)
-      
-      try:
-        files = os.listdir(city_folder)
-      except:
-        continue
-      
-      num_processed = 0
-      
-      for a_file in files:
-        full_file_path = os.path.join(city_folder, a_file)
-        f = open(full_file_path, 'r')
-        xml_data = f.read()
-        f.close()
-          
-        DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
-        
-        try:
-          longitude = float(DOMTree.getElementsByTagName('longitud')[0].firstChild.nodeValue)
-          latitude = float(DOMTree.getElementsByTagName('latitud')[0].firstChild.nodeValue)
-        except:
-          num_processed += 1
-          continue
-        
-        municipality = sanitize(DOMTree.getElementsByTagName('municipio')[0].firstChild.nodeValue)
-        province = sanitize(DOMTree.getElementsByTagName('provincia')[0].firstChild.nodeValue)
-        
-        id_input = categories_names[index_poi_type] + '-' + a_file + '-' + str(num_processed)
-        m = hashlib.md5()
-        m.update(id_input.decode())
-        
-        description_data = get_description(DOMTree, index_poi_type)
-        
-        name = description_data[0]
-        description = description_data[1]
-              
-        poi_entity = {
-          'id': categories_names[index_poi_type] + '-' + m.hexdigest(),
-          'type': 'PointOfInterest',
-          'name': {
-            'value': name
-          },
-          'description': {
-            'value': description
-          },
-          'category': {
-            'type': 'List',
-            'value': [
-              categories_values[index_poi_type]
-          ]},
-          'location': {
-            'type': 'geo:json',
-            'value': {
-              'type': 'Point',
-              'coordinates': [longitude, latitude]
-            }
-          },
-          'address': {
-            'type': 'PostalAddress',
-            'value': {
-              'addressRegion': province,
-              'addressLocality': municipality,
-              'addressCountry': 'ES'
-            }
-          },
-          'source': {
-            'type': 'URL',
-            'value': 'http://www.tourspain.es'
-          },
-          'dataProvider': {
-            'value': 'FIWARE Foundation e.V.'
-          }
-        }
-        
-        poi_list.append(poi_entity)
-        
-        num_processed += 1
-      
-    index_poi_type += 1
+    index_poi_type = 0
+
+    source_folder = source_folder_param
+    if source_folder is None:
+        source_folder = DEFAULT_SOURCE_FOLDER
+
+    for folder in folders:
+        poi_list = []
+        pois[categories_names[index_poi_type]] = poi_list
+
+        for city in cities:
+            city_folder = os.path.join(source_folder, folder, city)
+
+            try:
+                files = os.listdir(city_folder)
+            except BaseException:
+                continue
+
+            num_processed = 0
+
+            for a_file in files:
+                full_file_path = os.path.join(city_folder, a_file)
+                f = open(full_file_path, 'r')
+                xml_data = f.read()
+                f.close()
+
+                DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
+
+                try:
+                    longitude = float(DOMTree.getElementsByTagName(
+                        'longitud')[0].firstChild.nodeValue)
+                    latitude = float(DOMTree.getElementsByTagName(
+                        'latitud')[0].firstChild.nodeValue)
+                except BaseException:
+                    num_processed += 1
+                    continue
+
+                municipality = sanitize(
+                    DOMTree.getElementsByTagName('municipio')[0].firstChild.nodeValue)
+                province = sanitize(DOMTree.getElementsByTagName(
+                    'provincia')[0].firstChild.nodeValue)
+
+                id_input = categories_names[index_poi_type] + \
+                    '-' + a_file + '-' + str(num_processed)
+                m = hashlib.md5()
+                m.update(id_input.decode())
+
+                description_data = get_description(DOMTree, index_poi_type)
+
+                name = description_data[0]
+                description = description_data[1]
+
+                poi_entity = {
+                    'id': categories_names[index_poi_type] + '-' + m.hexdigest(),
+                    'type': 'PointOfInterest',
+                    'name': {
+                        'value': name
+                    },
+                    'description': {
+                        'value': description
+                    },
+                    'category': {
+                        'type': 'List',
+                        'value': [
+                            categories_values[index_poi_type]
+                        ]},
+                    'location': {
+                        'type': 'geo:json',
+                        'value': {
+                            'type': 'Point',
+                            'coordinates': [longitude, latitude]
+                        }
+                    },
+                    'address': {
+                        'type': 'PostalAddress',
+                        'value': {
+                            'addressRegion': province,
+                            'addressLocality': municipality,
+                            'addressCountry': 'ES'
+                        }
+                    },
+                    'source': {
+                        'type': 'URL',
+                        'value': 'http://www.tourspain.es'
+                    },
+                    'dataProvider': {
+                        'value': 'FIWARE Foundation e.V.'
+                    }
+                }
+
+                poi_list.append(poi_entity)
+
+                num_processed += 1
+
+        index_poi_type += 1
 
 
 # Obtains the POI description
 def get_description(DOMTree, index_poi_type):
-  description_node_name = description_nodes[index_poi_type]
-  description = ''
-  name = ''
-  
-  if description_node_name is None:
-    return description
-  
-  tipo_nodes = DOMTree.getElementsByTagName(description_node_name)
-  found = False
-  
-  for node in tipo_nodes:
-    if found is True:
-      break
-    
-    language = node.getAttribute('language')
-    
-    # Getting name
-    if (language == 'es' or language == 'en'):
-      candidate_name = node.getElementsByTagName('nombre')[1].firstChild.nodeValue
-      # Give priority to name in Spanish
-      if language == 'es' and len(candidate_name) > 0:
-        name = sanitize(candidate_name).strip()
-      elif language == 'en' and len(candidate_name) > 0 and len(name) == 0:
-        name = sanitize(candidate_name).strip()
-    
-    # Obtaining a description in English or Spanish
-    
-      content_nodes = node.getElementsByTagName('content')
-      for content_node in content_nodes:
-        if content_node.firstChild != None and content_node.firstChild.nodeValue != None:
-          text = content_node.firstChild.nodeValue
-          text_filtered = text.strip()
-          
-          m = re.search('^\<p\>(.*?)\<\/p\>', text)
-          if m != None:
-            text_filtered = m.group(1).strip()
-          else:
-            m2 = re.search('^p(.*?)\/p', text)
-            if m2 != None:
-              text_filtered = m.group(1).strip()
-
-          description = sanitize(text_filtered)
-          # There are more spureous tags in the description
-          # TODO: Remove any markup in the description by using an appropriate
-          # regular expression
-          description.replace('<strong>','').replace('</strong>','')
-          if len(description) > 0:
-            found = True
+    description_node_name = description_nodes[index_poi_type]
+    description = ''
+    name = ''
+
+    if description_node_name is None:
+        return description
+
+    tipo_nodes = DOMTree.getElementsByTagName(description_node_name)
+    found = False
+
+    for node in tipo_nodes:
+        if found is True:
             break
-       
-  return (name, description)
+
+        language = node.getAttribute('language')
+
+        # Getting name
+        if (language == 'es' or language == 'en'):
+            candidate_name = node.getElementsByTagName(
+                'nombre')[1].firstChild.nodeValue
+            # Give priority to name in Spanish
+            if language == 'es' and len(candidate_name) > 0:
+                name = sanitize(candidate_name).strip()
+            elif language == 'en' and len(candidate_name) > 0 and len(name) == 0:
+                name = sanitize(candidate_name).strip()
+
+        # Obtaining a description in English or Spanish
+
+            content_nodes = node.getElementsByTagName('content')
+            for content_node in content_nodes:
+                if content_node.firstChild is not None and content_node.firstChild.nodeValue is not None:
+                    text = content_node.firstChild.nodeValue
+                    text_filtered = text.strip()
+
+                    m = re.search('^\<p\>(.*?)\<\/p\>', text)
+                    if m is not None:
+                        text_filtered = m.group(1).strip()
+                    else:
+                        m2 = re.search('^p(.*?)\/p', text)
+                        if m2 is not None:
+                            text_filtered = m.group(1).strip()
+
+                    description = sanitize(text_filtered)
+                    # There are more spureous tags in the description
+                    # TODO: Remove any markup in the description by using an appropriate
+                    # regular expression
+                    description.replace(
+                        '<strong>', '').replace(
+                        '</strong>', '')
+                    if len(description) > 0:
+                        found = True
+                        break
+
+    return (name, description)
 
 
 def import_data():
-  for poi_type in pois:
-    poi_list = pois[poi_type]
-    
-    print(poi_type, len(poi_list))
-    
-    post_data(poi_list)
+    for poi_type in pois:
+        poi_list = pois[poi_type]
+
+        print(poi_type, len(poi_list))
+
+        post_data(poi_list)
 
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_data(data):
-  if len(data) == 0:
-    return
-  
-  payload = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  
-  data_as_str = json.dumps(payload)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SPATH
-  }
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-  
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      persisted_entities = persisted_entities + 1
-  except urllib2.URLError as e:
-    global in_error_entities
-    in_error_entities = in_error_entities + 1     
-
+    if len(data) == 0:
+        return
+
+    payload = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+
+    data_as_str = json.dumps(payload)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SPATH
+    }
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            persisted_entities = persisted_entities + 1
+    except urllib2.URLError as e:
+        global in_error_entities
+        in_error_entities = in_error_entities + 1
 
 
 if __name__ == '__main__':
-  transform_data(sys.argv[1])
-  import_data()
-  
-  print("Persisted entities: ", persisted_entities)
-  print("In error entities: ", in_error_entities)
+    transform_data(sys.argv[1])
+    import_data()
+
+    print("Persisted entities: ", persisted_entities)
+    print("In error entities: ", in_error_entities)
diff --git a/Weather/WeatherAlarm/meteoalarm.py b/Weather/WeatherAlarm/meteoalarm.py
index a364e44..82dd77e 100644
--- a/Weather/WeatherAlarm/meteoalarm.py
+++ b/Weather/WeatherAlarm/meteoalarm.py
@@ -13,155 +13,162 @@
 import json
 
 awareness_type_dict = {
-  '1':  'Wind',
-  '2':  'Snow/Ice',
-  '3':  'Thunderstorms',
-  '4':  'Fog',
-  '5':  'Extreme High Temperature',
-  '6':  'Extreme Low Temperature',
-  '7':  'Coastal Event',
-  '8':  'Forest Fire',
-  '9':  'Avalanches',
-  '10': 'Rain',
-  '11': 'Flood',
-  '12': 'Rain-Flood'
+    '1': 'Wind',
+    '2': 'Snow/Ice',
+    '3': 'Thunderstorms',
+    '4': 'Fog',
+    '5': 'Extreme High Temperature',
+    '6': 'Extreme Low Temperature',
+    '7': 'Coastal Event',
+    '8': 'Forest Fire',
+    '9': 'Avalanches',
+    '10': 'Rain',
+    '11': 'Flood',
+    '12': 'Rain-Flood'
 }
 
 awareness_level_dict = {
-  '':  'White',
-  '1': 'Green',
-  '2': 'Yellow',
-  '3': 'Orange',
-  '4': 'Red'
+    '': 'White',
+    '1': 'Green',
+    '2': 'Yellow',
+    '3': 'Orange',
+    '4': 'Red'
 }
 
-weather_alarms   = "http://www.meteoalarm.eu/documents/rss/{}.rss"
+weather_alarms = "http://www.meteoalarm.eu/documents/rss/{}.rss"
 
 reg_exp = re.compile('<img(?P<group>.*?)>')
 
+
 def get_weather_alarms(request):
-  query = request.args.get('q')
-
-  if not query:
-      return Response(json.dumps([]), mimetype='application/json')
-  
-  tokens  = query.split(';')
-  
-  country = ''
-  
-  for token in tokens:
-    items = token.split(':')
-    if items[0] == 'country':
-      country = items[1].lower()
-  
-  source = weather_alarms.format(country)
-  req = urllib2.Request(url=source)
-  f = urllib2.urlopen(req)
-  
-  xml_data = f.read()
-  final_data = xml_data
-  DOMTree = xml.dom.minidom.parseString(final_data).documentElement
-  
-  out = []
-  
-  items = DOMTree.getElementsByTagName('item')[1:]
-  
-  alarm_index = -1
-   
-  for item in items:
-    description = item.getElementsByTagName('description')[0].firstChild.nodeValue
-    # Enable description parsing
-    description = description.replace('&nbsp;', '')
-    description = re.sub(reg_exp,'<img\g<group>></img>',description)
-    
-    zone = item.getElementsByTagName('title')[0].firstChild.nodeValue.strip()
-    uid = item.getElementsByTagName('guid')[0].firstChild.nodeValue
-    pub_date_str = item.getElementsByTagName('pubDate')[0].firstChild.nodeValue
-    pub_date = datetime.datetime.strptime(pub_date_str[:-6], '%a, %d %b %Y %H:%M:%S').isoformat()
-    
-    # It is needed to encode description as it is already unicode
-    parsed_content = xml.dom.minidom.parseString(description.encode('utf-8')).documentElement
-    rows = parsed_content.getElementsByTagName('tr')
-    
-    for row in rows:
-      columns = row.getElementsByTagName('td')
-      for column in columns:
-        # img column contains the awareness level and type
-        img_aux = column.getElementsByTagName('img')
-        if img_aux.length > 0:
-          awareness_str = img_aux[0].getAttribute('alt')
-          alarm_data = parse_alarm(awareness_str)
-          
-          if alarm_data['level'] > 1:
-            alarm_index += 1
-            obj = {
-              'type': 'WeatherAlarm',
-              'id':   'WeatherAlarm' + '-' + uid + '-' + str(alarm_index),
-              'validity': {
-                'from': '',
-                'to':   ''
-              },
-              'awarenessType':  alarm_data['awt'],
-              'awarenessLevel': alarm_data['levelColor'],
-              'address': {
-                'addressCountry': country.upper(),
-                'addressRegion': zone
-              },
-              'source': 'http://www.meteoalarm.eu',
-              'dateCreated': pub_date
-            }
-            out.append(obj)
-        else:
-          dates = column.getElementsByTagName('i')
-          if dates.length > 0:
-            valid_from_str = dates[0].firstChild.nodeValue
-            valid_to_str = dates[1].firstChild.nodeValue
-            
-            valid_from = datetime.datetime.strptime(valid_from_str, '%d.%m.%Y %H:%M %Z').isoformat()
-            valid_to = datetime.datetime.strptime(valid_to_str, '%d.%m.%Y %H:%M %Z').isoformat()
-            
-            out[alarm_index]['validity']['from'] = valid_from
-            out[alarm_index]['validity']['to'] = valid_to
-  
-  out = remove_duplicates(out)
-  return Response(json.dumps(out), mimetype='application/json')
+    query = request.args.get('q')
+
+    if not query:
+        return Response(json.dumps([]), mimetype='application/json')
+
+    tokens = query.split(';')
+
+    country = ''
+
+    for token in tokens:
+        items = token.split(':')
+        if items[0] == 'country':
+            country = items[1].lower()
+
+    source = weather_alarms.format(country)
+    req = urllib2.Request(url=source)
+    f = urllib2.urlopen(req)
+
+    xml_data = f.read()
+    final_data = xml_data
+    DOMTree = xml.dom.minidom.parseString(final_data).documentElement
+
+    out = []
+
+    items = DOMTree.getElementsByTagName('item')[1:]
+
+    alarm_index = -1
+
+    for item in items:
+        description = item.getElementsByTagName(
+            'description')[0].firstChild.nodeValue
+        # Enable description parsing
+        description = description.replace('&nbsp;', '')
+        description = re.sub(reg_exp, '<img\g<group>></img>', description)
+
+        zone = item.getElementsByTagName(
+            'title')[0].firstChild.nodeValue.strip()
+        uid = item.getElementsByTagName('guid')[0].firstChild.nodeValue
+        pub_date_str = item.getElementsByTagName(
+            'pubDate')[0].firstChild.nodeValue
+        pub_date = datetime.datetime.strptime(
+            pub_date_str[:-6], '%a, %d %b %Y %H:%M:%S').isoformat()
+
+        # It is needed to encode description as it is already unicode
+        parsed_content = xml.dom.minidom.parseString(
+            description.encode('utf-8')).documentElement
+        rows = parsed_content.getElementsByTagName('tr')
+
+        for row in rows:
+            columns = row.getElementsByTagName('td')
+            for column in columns:
+                # img column contains the awareness level and type
+                img_aux = column.getElementsByTagName('img')
+                if img_aux.length > 0:
+                    awareness_str = img_aux[0].getAttribute('alt')
+                    alarm_data = parse_alarm(awareness_str)
+
+                    if alarm_data['level'] > 1:
+                        alarm_index += 1
+                        obj = {
+                            'type': 'WeatherAlarm',
+                            'id': 'WeatherAlarm' + '-' + uid + '-' + str(alarm_index),
+                            'validity': {
+                                'from': '',
+                                'to': ''},
+                            'awarenessType': alarm_data['awt'],
+                            'awarenessLevel': alarm_data['levelColor'],
+                            'address': {
+                                'addressCountry': country.upper(),
+                                'addressRegion': zone},
+                            'source': 'http://www.meteoalarm.eu',
+                            'dateCreated': pub_date}
+                        out.append(obj)
+                else:
+                    dates = column.getElementsByTagName('i')
+                    if dates.length > 0:
+                        valid_from_str = dates[0].firstChild.nodeValue
+                        valid_to_str = dates[1].firstChild.nodeValue
+
+                        valid_from = datetime.datetime.strptime(
+                            valid_from_str, '%d.%m.%Y %H:%M %Z').isoformat()
+                        valid_to = datetime.datetime.strptime(
+                            valid_to_str, '%d.%m.%Y %H:%M %Z').isoformat()
+
+                        out[alarm_index]['validity']['from'] = valid_from
+                        out[alarm_index]['validity']['to'] = valid_to
+
+    out = remove_duplicates(out)
+    return Response(json.dumps(out), mimetype='application/json')
+
 
 def remove_duplicates(array_data):
-  # Dictionary for duplicate checking
-  alarms_duplicates = { }
-  out = []
-  
-  for data in array_data:
-    key = data['address']['addressCountry'] + data['address']['addressRegion'] +\
-    data['awarenessLevel'] + data['awarenessType'] +\
-    data['validity']['from'] + data['validity']['to']
-    
-    if not key in alarms_duplicates:
-      alarms_duplicates[key] = data
-      out.append(data)
-  
-  return out
+    # Dictionary for duplicate checking
+    alarms_duplicates = {}
+    out = []
+
+    for data in array_data:
+        key = data['address']['addressCountry'] + data['address']['addressRegion'] +\
+            data['awarenessLevel'] + data['awarenessType'] +\
+            data['validity']['from'] + data['validity']['to']
+
+        if key not in alarms_duplicates:
+            alarms_duplicates[key] = data
+            out.append(data)
+
+    return out
+
 
 def parse_alarm(alarm_string):
-  elements = alarm_string.split(' ')
-  awt = elements[0].split(':')[1]
-  level = elements[1].split(':')[1]
-  
-  if level:
-    level_num = int(level)
-  else:
-    level_num = -1
-  
-  out = {
-    'level':     level_num,
-    'levelColor': '',
-    'awt':        ''
-  }
-  
-  if level in awareness_level_dict:
-    out['levelColor'] = awareness_level_dict[level]
-  
-  if awt in awareness_type_dict:
-    out['awt'] = awareness_type_dict[awt]
-    
-  return out
+    elements = alarm_string.split(' ')
+    awt = elements[0].split(':')[1]
+    level = elements[1].split(':')[1]
+
+    if level:
+        level_num = int(level)
+    else:
+        level_num = -1
+
+    out = {
+        'level': level_num,
+        'levelColor': '',
+        'awt': ''
+    }
+
+    if level in awareness_level_dict:
+        out['levelColor'] = awareness_level_dict[level]
+
+    if awt in awareness_type_dict:
+        out['awt'] = awareness_type_dict[awt]
+
+    return out
diff --git a/Weather/WeatherForecast/harvest/aemet.py b/Weather/WeatherForecast/harvest/aemet.py
index e862aa6..edfa9f1 100644
--- a/Weather/WeatherForecast/harvest/aemet.py
+++ b/Weather/WeatherForecast/harvest/aemet.py
@@ -17,245 +17,264 @@
 from weather_observed import get_weather_observed
 
 postal_codes = {
-  '47001': '47186',
-  '28001': '28079',
-  '39001': '39075',
-  '34001': '34120',
-  '34200': '34023',
-  '05194': '05123',
-  '33300': '33076',
-  '41001': '41091',
-  '46005': '46250'
+    '47001': '47186',
+    '28001': '28079',
+    '39001': '39075',
+    '34001': '34120',
+    '34200': '34023',
+    '05194': '05123',
+    '33300': '33076',
+    '41001': '41091',
+    '46005': '46250'
 }
 
 localities = {
-  'Valladolid':         '47186',
-  'Madrid':             '28079',
-  'Santander':          '39075',
-  'Palencia':           '34120',
-  u'Venta de Ba帽os':    '34023',
-  'Mediana de Voltoya': '05123',
-  'Villaviciosa':       '33076',
-  'Sevilla':            '41091',
-  'Valencia':           '46250'
+    'Valladolid': '47186',
+    'Madrid': '28079',
+    'Santander': '39075',
+    'Palencia': '34120',
+    u'Venta de Ba帽os': '34023',
+    'Mediana de Voltoya': '05123',
+    'Villaviciosa': '33076',
+    'Sevilla': '41091',
+    'Valencia': '46250'
 }
 
 app = Flask(__name__)
 
-aemet_service    = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+aemet_service = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+
 
 @app.route('/')
 def index():
     return "Hello, World!"
 
-  
-@app.route('/v2/entities',  methods=['GET'])
+
+@app.route('/v2/entities', methods=['GET'])
 def get_weather():
     entity_type = request.args.get('type')
-      
+
     if entity_type == 'WeatherForecast':
-      return get_weather_forecasted(request)
+        return get_weather_forecasted(request)
     elif entity_type == 'WeatherObserved':
-      return get_weather_observed(request)
+        return get_weather_observed(request)
     elif entity_type == 'WeatherAlarm':
-      return get_weather_alarms(request)
+        return get_weather_alarms(request)
     else:
-      return Response(json.dumps([]), mimetype='application/json')
-    
-  
+        return Response(json.dumps([]), mimetype='application/json')
+
+
 def get_data(row, index, conversion=float, factor=1.0):
-  out = None
-  
-  value = row[index]
-  if(value != ''):
-    out = conversion(value) / factor
-    
-  return out
-    
-def get_weather_forecasted(request):    
+    out = None
+
+    value = row[index]
+    if(value != ''):
+        out = conversion(value) / factor
+
+    return out
+
+
+def get_weather_forecasted(request):
     country = ''
     postal_code = ''
     address_locality = ''
-    
+
     query = request.args.get('q')
-    
+
     if not query:
-      return Response(json.dumps([]), mimetype='application/json')
-    
-    tokens  = query.split(';')
+        return Response(json.dumps([]), mimetype='application/json')
+
+    tokens = query.split(';')
     for token in tokens:
-      items = token.split(':')
-      if items[0] == 'postalCode':
-        postal_code = items[1]
-      elif items[0] == 'country':
-        country = items[1]
-      elif items[0] == 'addressLocality':
-        address_locality = items[1]
-        
+        items = token.split(':')
+        if items[0] == 'postalCode':
+            postal_code = items[1]
+        elif items[0] == 'country':
+            country = items[1]
+        elif items[0] == 'addressLocality':
+            address_locality = items[1]
+
     if country == 'PT' and address_locality:
-      return Response(json.dumps(get_weather_forecasted_pt(address_locality)), mimetype='application/json')
-    
-    if not country or (not postal_code in postal_codes and not address_locality in localities) or country != 'ES':
-      return Response(json.dumps([]), mimetype='application/json')
-    
+        return Response(
+            json.dumps(
+                get_weather_forecasted_pt(address_locality)),
+            mimetype='application/json')
+
+    if not country or (
+            postal_code in postal_codes and not address_locality not in localities) or country != 'ES':
+        return Response(json.dumps([]), mimetype='application/json')
+
     param = ''
     if postal_code:
-      param = postal_codes[postal_code]
+        param = postal_codes[postal_code]
     elif address_locality:
-      param = localities[address_locality]
-    
+        param = localities[address_locality]
+
     source = aemet_service.format(param)
     req = urllib2.Request(url=source)
     f = urllib2.urlopen(req)
     xml_data = f.read()
     DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
-    
-    address_locality = DOMTree.getElementsByTagName('nombre')[0].firstChild.nodeValue
-    address = { }
+
+    address_locality = DOMTree.getElementsByTagName(
+        'nombre')[0].firstChild.nodeValue
+    address = {}
     address['addressCountry'] = country
     address['postalCode'] = postal_code
     address['addressLocality'] = address_locality
-    
-    created =  DOMTree.getElementsByTagName('elaborado')[0].firstChild.nodeValue
-    
-    forecasts = DOMTree.getElementsByTagName('prediccion')[0].getElementsByTagName('dia')
-    
+
+    created = DOMTree.getElementsByTagName('elaborado')[0].firstChild.nodeValue
+
+    forecasts = DOMTree.getElementsByTagName(
+        'prediccion')[0].getElementsByTagName('dia')
+
     out = []
     for forecast in forecasts:
-      date = forecast.getAttribute('fecha')
-      normalizedForecast = parse_aemet_forecast(forecast, date)
-      counter = 1
-      for f in normalizedForecast:
-        f['type'] = 'WeatherForecast'
-        f['id'] = generate_id(postal_code, country, date) + '_' + str(counter)
-        f['address'] = address
-        f['dateCreated'] = created
-        f['source'] = source
-        counter+=1
-        out.append(f)
-    
+        date = forecast.getAttribute('fecha')
+        normalizedForecast = parse_aemet_forecast(forecast, date)
+        counter = 1
+        for f in normalizedForecast:
+            f['type'] = 'WeatherForecast'
+            f['id'] = generate_id(
+                postal_code, country, date) + '_' + str(counter)
+            f['address'] = address
+            f['dateCreated'] = created
+            f['source'] = source
+            counter += 1
+            out.append(f)
+
     return Response(json.dumps(out), mimetype='application/json')
-    
+
 
 def parse_aemet_forecast(forecast, date):
-  periods = { }
-  out = []
-  
-  parsed_date = parser.parse(date)
-  
-  pops = forecast.getElementsByTagName('prob_precipitacion')
-  for pop in pops:
-    period = pop.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if pop.firstChild and pop.firstChild.nodeValue:
-      insert_into_period(periods, period,
-                  'precipitationProbability', float(pop.firstChild.nodeValue) / 100.0)
-  
-  period = None
-  weather_types = forecast.getElementsByTagName('estado_cielo')
-  for weather_type in weather_types:
-    period = weather_type.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if weather_type.firstChild and weather_type.firstChild.nodeValue:
-      insert_into_period(periods, period, 'weatherType',
-                         weather_type.getAttribute('descripcion'))
-  
-  period = None
-  wind_data = forecast.getElementsByTagName('viento')
-  for wind in wind_data:
-    period = wind.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    wind_direction = wind.getElementsByTagName('direccion')[0]
-    wind_speed = wind.getElementsByTagName('velocidad')[0]
-    if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windSpeed',
-                         int(wind_speed.firstChild.nodeValue))
-    if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windDirection',
-                         wind_direction.firstChild.nodeValue)
-  
-  temperature_node = forecast.getElementsByTagName('temperatura')[0]
-  max_temp = float(temperature_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp = float(temperature_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temperature_node, periods, 'temperature')
-  
-  temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
-  max_temp_feels = float(temp_feels_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp_feels = float(temp_feels_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
-  
-  humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
-  max_humidity = float(humidity_node.getElementsByTagName('maxima')[0].firstChild.nodeValue) / 100.0
-  min_humidity = float(humidity_node.getElementsByTagName('minima')[0].firstChild.nodeValue) / 100.0
-  get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
-  
-  for period in periods:
-    print(period)
-  
-  for period in periods:
-    period_items = period.split('-')
-    period_start = period_items[0]
-    period_end = period_items[1]
-    end_hour = int(period_end)
-    end_date = copy.deepcopy(parsed_date)
-    if end_hour > 23:
-      end_hour = 0
-      end_date = parsed_date + datetime.timedelta(days=1)
-    
-    start_date = parsed_date.replace(hour=int(period_start), minute=0, second=0)
-    end_date = end_date.replace(hour=end_hour,minute=0,second=0)   
-    
-    objPeriod = periods[period]
-    objPeriod['validity'] = { }
-    objPeriod['validity']['from'] = start_date.isoformat()
-    objPeriod['validity']['to'] = end_date.isoformat()
-    
-    maximum = { }
-    objPeriod['dayMaximum'] = maximum
-    minimum = { }
-    objPeriod['dayMinimum'] = minimum
-    
-    maximum['temperature'] = max_temp
-    minimum['temperature'] = min_temp
-    
-    maximum['relativeHumidity'] = max_humidity
-    minimum['relativeHumidity'] = min_humidity
-    
-    maximum['feelsLikeTemperature'] = max_temp_feels
-    minimum['feelsLikeTemperature'] = min_temp_feels
-    
-    out.append(objPeriod)
-  
-  return out
+    periods = {}
+    out = []
+
+    parsed_date = parser.parse(date)
+
+    pops = forecast.getElementsByTagName('prob_precipitacion')
+    for pop in pops:
+        period = pop.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if pop.firstChild and pop.firstChild.nodeValue:
+            insert_into_period(
+                periods, period, 'precipitationProbability', float(
+                    pop.firstChild.nodeValue) / 100.0)
+
+    period = None
+    weather_types = forecast.getElementsByTagName('estado_cielo')
+    for weather_type in weather_types:
+        period = weather_type.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if weather_type.firstChild and weather_type.firstChild.nodeValue:
+            insert_into_period(periods, period, 'weatherType',
+                               weather_type.getAttribute('descripcion'))
+
+    period = None
+    wind_data = forecast.getElementsByTagName('viento')
+    for wind in wind_data:
+        period = wind.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        wind_direction = wind.getElementsByTagName('direccion')[0]
+        wind_speed = wind.getElementsByTagName('velocidad')[0]
+        if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
+            insert_into_period(periods, period, 'windSpeed',
+                               int(wind_speed.firstChild.nodeValue))
+        if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
+            insert_into_period(periods, period, 'windDirection',
+                               wind_direction.firstChild.nodeValue)
+
+    temperature_node = forecast.getElementsByTagName('temperatura')[0]
+    max_temp = float(temperature_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp = float(temperature_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temperature_node, periods, 'temperature')
+
+    temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
+    max_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
+
+    humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
+    max_humidity = float(humidity_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue) / 100.0
+    min_humidity = float(humidity_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue) / 100.0
+    get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
+
+    for period in periods:
+        print(period)
+
+    for period in periods:
+        period_items = period.split('-')
+        period_start = period_items[0]
+        period_end = period_items[1]
+        end_hour = int(period_end)
+        end_date = copy.deepcopy(parsed_date)
+        if end_hour > 23:
+            end_hour = 0
+            end_date = parsed_date + datetime.timedelta(days=1)
+
+        start_date = parsed_date.replace(
+            hour=int(period_start), minute=0, second=0)
+        end_date = end_date.replace(hour=end_hour, minute=0, second=0)
+
+        objPeriod = periods[period]
+        objPeriod['validity'] = {}
+        objPeriod['validity']['from'] = start_date.isoformat()
+        objPeriod['validity']['to'] = end_date.isoformat()
+
+        maximum = {}
+        objPeriod['dayMaximum'] = maximum
+        minimum = {}
+        objPeriod['dayMinimum'] = minimum
+
+        maximum['temperature'] = max_temp
+        minimum['temperature'] = min_temp
+
+        maximum['relativeHumidity'] = max_humidity
+        minimum['relativeHumidity'] = min_humidity
+
+        maximum['feelsLikeTemperature'] = max_temp_feels
+        minimum['feelsLikeTemperature'] = min_temp_feels
+
+        out.append(objPeriod)
+
+    return out
 
 
 def get_parameter_data(node, periods, parameter, factor=1.0):
-  param_periods = node.getElementsByTagName('dato')
-  for param in param_periods:
-    hour_str = param.getAttribute('hora')
-    hour = int(hour_str)
-    interval_start = hour - 6
-    interval_start_str = str(interval_start)
-    if interval_start < 10:
-      interval_start_str = '0' + str(interval_start)
-      
-    period = interval_start_str + '-' + hour_str
-    if param.firstChild and param.firstChild.nodeValue:
-      param_val = float(param.firstChild.nodeValue)
-      insert_into_period(periods, period, parameter, param_val / factor)
+    param_periods = node.getElementsByTagName('dato')
+    for param in param_periods:
+        hour_str = param.getAttribute('hora')
+        hour = int(hour_str)
+        interval_start = hour - 6
+        interval_start_str = str(interval_start)
+        if interval_start < 10:
+            interval_start_str = '0' + str(interval_start)
+
+        period = interval_start_str + '-' + hour_str
+        if param.firstChild and param.firstChild.nodeValue:
+            param_val = float(param.firstChild.nodeValue)
+            insert_into_period(periods, period, parameter, param_val / factor)
 
 
 def insert_into_period(periods, period, attribute, value):
-  if not period in periods:
-    periods[period] = { }
-  
-  periods[period][attribute] = value
+    if period not in periods:
+        periods[period] = {}
+
+    periods[period][attribute] = value
+
 
 def generate_id(postal_code, country, date):
-  return postal_code + '_' + country + '_' + date
+    return postal_code + '_' + country + '_' + date
+
 
 if __name__ == '__main__':
-    app.run(host='0.0.0.0',port=1028,debug=True)
+    app.run(host='0.0.0.0', port=1028, debug=True)
diff --git a/Weather/WeatherForecast/harvest/ipma.py b/Weather/WeatherForecast/harvest/ipma.py
index d176603..d02036b 100644
--- a/Weather/WeatherForecast/harvest/ipma.py
+++ b/Weather/WeatherForecast/harvest/ipma.py
@@ -11,124 +11,126 @@
 import sys
 
 iptma_codes = {
-  'Lisboa': '1110600',
-  'Porto':  '1131200',
-  'Aveiro': '1010500'
+    'Lisboa': '1110600',
+    'Porto': '1131200',
+    'Aveiro': '1010500'
 }
 
 weather_type_dict = {
-  '1': 'Clear',
-  '2': 'Slightly cloudy',
-  '3': 'Partly Cloudy',
-  '4': 'Overcast',
-  '5': 'High clouds',
-  '6': 'Light rain',
-  '7': 'Drizzle',
-  '11': 'Heavy rain',
-  '9':  'Rain'
+    '1': 'Clear',
+    '2': 'Slightly cloudy',
+    '3': 'Partly Cloudy',
+    '4': 'Overcast',
+    '5': 'High clouds',
+    '6': 'Light rain',
+    '7': 'Drizzle',
+    '11': 'Heavy rain',
+    '9': 'Rain'
 }
 
 ipma_url = 'https://api.ipma.pt/json/alldata/{}.json'
 
+
 def get_weather_forecasted_pt(locality):
-  source = ipma_url.format(iptma_codes[locality])
-  
-  req = urllib2.Request(url=source)
-  f = urllib2.urlopen(req)
-  data = json.loads(f.read())
-  
-  print(data)
-  out = []
-  maxMinDay = { }
-    
-  for forecast in data:
-    tMax = get_data(forecast, 'tMax')
-    tMin = get_data(forecast, 'tMin')
-    
-    valid_from = parser.parse(forecast['dataPrev'])
-    period = int(forecast['idPeriodo'])
-    valid_to = valid_from + datetime.timedelta(hours=period)
-    
-    day = valid_from.date()
-    key = day.isoformat()
-    
-    if key not in maxMinDay:
-      maxMinDay[key] = { }
-      
-    if period == 24:
-      maxMinDay[key]['tMax'] = tMax
-      maxMinDay[key]['tMin'] = tMin
-    
-    if tMax is None:
-      if 'tMax' in maxMinDay[key]:
-        tMax =  maxMinDay[key]['tMax']  
-  
-    if tMin is None:
-      if 'tMin' in maxMinDay[key]:
-        tMin = maxMinDay[key]['tMin']
-    
-    now = datetime.datetime.now()
-    
-    if valid_to < now:
-      continue
-    
-    # Only today's forecast
-    day_now = now.date()
-    day_wf = valid_to.date()
-    
-    if day_now != day_wf:
-      continue
-    
-    obj = {
-      'type': 'WeatherForecast',
-      'feelsLikeTemperature': get_data(forecast, 'utci'),
-      'temperature': get_data(forecast, 'tMed'),
-    }
-    
-    if tMax is not None:
-      obj['dayMaximum'] = {
-        'temperature': tMax
-      }
-    
-    if tMin is not None:
-      obj['dayMinimum'] = {
-        'temperature': tMin
-      }
-    
-    hr = get_data(forecast, 'hR')
-    if hr != None:
-      hr = hr / 100
-    else:
-      hr = None
-      
-    obj['relativeHumidity'] = hr
-    obj['dateCreated'] = forecast['dataUpdate']
-    obj['validity'] = {
-      'from': forecast['dataPrev'],
-      'to':   valid_to.isoformat()
-    }
-    obj['address'] = {
-      'addressCountry': 'PT',
-      'addressLocality': locality
-    }
-    
-    obj['windDirection'] = forecast['ddVento']
-    obj['windSpeed'] = float(forecast['ffVento'])
-    
-    weather_type_id = str(forecast['idTipoTempo'])
-    if weather_type_id in weather_type_dict:
-      obj['weatherType'] = weather_type_dict[weather_type_id]
-      
-    obj['id'] = 'PT' + '-'  + locality + '-' + obj['validity']['from'] + '-' + obj['validity']['to']
-      
-    out.append(obj)
-  
-  return out
+    source = ipma_url.format(iptma_codes[locality])
+
+    req = urllib2.Request(url=source)
+    f = urllib2.urlopen(req)
+    data = json.loads(f.read())
+
+    print(data)
+    out = []
+    maxMinDay = {}
+
+    for forecast in data:
+        tMax = get_data(forecast, 'tMax')
+        tMin = get_data(forecast, 'tMin')
+
+        valid_from = parser.parse(forecast['dataPrev'])
+        period = int(forecast['idPeriodo'])
+        valid_to = valid_from + datetime.timedelta(hours=period)
+
+        day = valid_from.date()
+        key = day.isoformat()
+
+        if key not in maxMinDay:
+            maxMinDay[key] = {}
+
+        if period == 24:
+            maxMinDay[key]['tMax'] = tMax
+            maxMinDay[key]['tMin'] = tMin
+
+        if tMax is None:
+            if 'tMax' in maxMinDay[key]:
+                tMax = maxMinDay[key]['tMax']
+
+        if tMin is None:
+            if 'tMin' in maxMinDay[key]:
+                tMin = maxMinDay[key]['tMin']
+
+        now = datetime.datetime.now()
+
+        if valid_to < now:
+            continue
+
+        # Only today's forecast
+        day_now = now.date()
+        day_wf = valid_to.date()
+
+        if day_now != day_wf:
+            continue
+
+        obj = {
+            'type': 'WeatherForecast',
+            'feelsLikeTemperature': get_data(forecast, 'utci'),
+            'temperature': get_data(forecast, 'tMed'),
+        }
+
+        if tMax is not None:
+            obj['dayMaximum'] = {
+                'temperature': tMax
+            }
+
+        if tMin is not None:
+            obj['dayMinimum'] = {
+                'temperature': tMin
+            }
+
+        hr = get_data(forecast, 'hR')
+        if hr is not None:
+            hr = hr / 100
+        else:
+            hr = None
+
+        obj['relativeHumidity'] = hr
+        obj['dateCreated'] = forecast['dataUpdate']
+        obj['validity'] = {
+            'from': forecast['dataPrev'],
+            'to': valid_to.isoformat()
+        }
+        obj['address'] = {
+            'addressCountry': 'PT',
+            'addressLocality': locality
+        }
+
+        obj['windDirection'] = forecast['ddVento']
+        obj['windSpeed'] = float(forecast['ffVento'])
+
+        weather_type_id = str(forecast['idTipoTempo'])
+        if weather_type_id in weather_type_dict:
+            obj['weatherType'] = weather_type_dict[weather_type_id]
+
+        obj['id'] = 'PT' + '-' + locality + '-' + \
+            obj['validity']['from'] + '-' + obj['validity']['to']
+
+        out.append(obj)
+
+    return out
 
 
 def get_data(forecast, item):
-  value = float(forecast[item])
-  if value == -99.0:
-    value = None
-  
-  return value
+    value = float(forecast[item])
+    if value == -99.0:
+        value = None
+
+    return value
diff --git a/Weather/WeatherForecast/harvest/portugal_weather_forecast_harvest.py b/Weather/WeatherForecast/harvest/portugal_weather_forecast_harvest.py
index be3b9d1..281bf85 100644
--- a/Weather/WeatherForecast/harvest/portugal_weather_forecast_harvest.py
+++ b/Weather/WeatherForecast/harvest/portugal_weather_forecast_harvest.py
@@ -27,260 +27,278 @@
 
 MIME_JSON = 'application/json'
 FIWARE_SERVICE = 'Weather'
-FIWARE_SPATH =   '/Portugal'
+FIWARE_SPATH = '/Portugal'
 
 
 iptma_codes = {
-  'Lisboa': '1110600',
-  'Porto':  '1131200',
-  'Aveiro': '1010500'
+    'Lisboa': '1110600',
+    'Porto': '1131200',
+    'Aveiro': '1010500'
 }
 
 
 def decode_wind_direction(direction):
-  dictionary = {
-    'N': 180,
-    'S': 0,
-    'E': -90,
-    'W': 90,
-    'NE': -45,
-    'NW': 45,
-    'SE': -135,
-    'SW': 135
-  }
-  
-  if direction in dictionary: 
-    return dictionary[direction]
-  else: return None
+    dictionary = {
+        'N': 180,
+        'S': 0,
+        'E': -90,
+        'W': 90,
+        'NE': -45,
+        'NW': 45,
+        'SE': -135,
+        'SW': 135
+    }
 
+    if direction in dictionary:
+        return dictionary[direction]
+    else:
+        return None
 
 
 weather_type_dict = {
-  '1': 'clear',
-  '2': 'slightlyCloudy',
-  '3': 'partlyCloudy',
-  '4': 'overcast',
-  '5': 'highClouds',
-  '6': 'lightRain',
-  '7': 'drizzle',
-  '11': 'heavyRain',
-  '9':  'rain'
+    '1': 'clear',
+    '2': 'slightlyCloudy',
+    '3': 'partlyCloudy',
+    '4': 'overcast',
+    '5': 'highClouds',
+    '6': 'lightRain',
+    '7': 'drizzle',
+    '11': 'heavyRain',
+    '9': 'rain'
 }
 
 ipma_url = 'https://api.ipma.pt/json/alldata/{}.json'
 
+
 def get_weather_forecasted():
-  data = { }
-  
-  for locality in iptma_codes:
-    code = iptma_codes[locality]
-    
-    data[code] = get_weather_forecasted_pt(locality)
-    
-  return data
+    data = {}
+
+    for locality in iptma_codes:
+        code = iptma_codes[locality]
+
+        data[code] = get_weather_forecasted_pt(locality)
+
+    return data
 
 
 def get_weather_forecasted_pt(locality):
-  source = ipma_url.format(iptma_codes[locality])
-  
-  req = urllib2.Request(url=source)
-  f = urllib2.urlopen(req)
-  data = json.loads(f.read())
-  
-  out = []
-  maxMinDay = { }
-    
-  for forecast in data:
-    tMax = get_data(forecast, 'tMax')
-    tMin = get_data(forecast, 'tMin')
-    
-    valid_from = parser.parse(forecast['dataPrev'])
-    period = int(forecast['idPeriodo'])
-    valid_to = valid_from + datetime.timedelta(hours=period)
-    
-    day = valid_from.date()
-    key = day.isoformat()
-    
-    if key not in maxMinDay:
-      maxMinDay[key] = { }
-      
-    if period == 24:
-      maxMinDay[key]['tMax'] = tMax
-      maxMinDay[key]['tMin'] = tMin
-    
-    if tMax is None:
-      if 'tMax' in maxMinDay[key]:
-        tMax =  maxMinDay[key]['tMax']  
-  
-    if tMin is None:
-      if 'tMin' in maxMinDay[key]:
-        tMin = maxMinDay[key]['tMin']
-    
-    now = datetime.datetime.now()
-    
-    if valid_to < now:
-      continue
-    
-    # Only today's forecast
-    day_now = now.date()
-    day_wf = valid_to.date()
-    
-    if day_now != day_wf:
-      continue
-    
-    obj = {
-      'type': 'WeatherForecast',
-      'feelsLikeTemperature': {
-        'value': get_data(forecast, 'utci')
-      },
-      'temperature': {
-        'value': get_data(forecast, 'tMed')
-      }
-    }
-    
-    if tMax is not None:
-      obj['dayMaximum'] = {
-        'value': {
-          'temperature': tMax
+    source = ipma_url.format(iptma_codes[locality])
+
+    req = urllib2.Request(url=source)
+    f = urllib2.urlopen(req)
+    data = json.loads(f.read())
+
+    out = []
+    maxMinDay = {}
+
+    for forecast in data:
+        tMax = get_data(forecast, 'tMax')
+        tMin = get_data(forecast, 'tMin')
+
+        valid_from = parser.parse(forecast['dataPrev'])
+        period = int(forecast['idPeriodo'])
+        valid_to = valid_from + datetime.timedelta(hours=period)
+
+        day = valid_from.date()
+        key = day.isoformat()
+
+        if key not in maxMinDay:
+            maxMinDay[key] = {}
+
+        if period == 24:
+            maxMinDay[key]['tMax'] = tMax
+            maxMinDay[key]['tMin'] = tMin
+
+        if tMax is None:
+            if 'tMax' in maxMinDay[key]:
+                tMax = maxMinDay[key]['tMax']
+
+        if tMin is None:
+            if 'tMin' in maxMinDay[key]:
+                tMin = maxMinDay[key]['tMin']
+
+        now = datetime.datetime.now()
+
+        if valid_to < now:
+            continue
+
+        # Only today's forecast
+        day_now = now.date()
+        day_wf = valid_to.date()
+
+        if day_now != day_wf:
+            continue
+
+        obj = {
+            'type': 'WeatherForecast',
+            'feelsLikeTemperature': {
+                'value': get_data(forecast, 'utci')
+            },
+            'temperature': {
+                'value': get_data(forecast, 'tMed')
+            }
         }
-      }
-    
-    if tMin is not None:
-      obj['dayMinimum'] = {
-        'value': {
-          'temperature': tMin
+
+        if tMax is not None:
+            obj['dayMaximum'] = {
+                'value': {
+                    'temperature': tMax
+                }
+            }
+
+        if tMin is not None:
+            obj['dayMinimum'] = {
+                'value': {
+                    'temperature': tMin
+                }
+            }
+
+        hr = get_data(forecast, 'hR')
+        if hr is not None:
+            hr = hr / 100
+        else:
+            hr = None
+
+        obj['relativeHumidity'] = {
+            'value': hr
+        }
+        obj['dateIssued'] = {
+            'value': forecast['dataUpdate'],
+            'type': 'DateTime'
+        }
+        obj['dateRetrieved'] = {
+            'type': 'DateTime',
+            'value': datetime.datetime.now(lisbon_tz).replace(
+                microsecond=0).isoformat()}
+        obj['validFrom'] = {
+            'value': forecast['dataPrev'],
+            'type': 'DateTime'
+        }
+        obj['validTo'] = {
+            'value': valid_to.isoformat(),
+            'type': 'DateTime'
+        }
+        obj['validity'] = {
+            'value': obj['validFrom']['value'] + '/' + obj['validTo']['value']
+        }
+        obj['address'] = {
+            'value': {
+                'addressCountry': 'PT',
+                'addressLocality': locality
+            },
+            'type': 'PostalAddress'
+        }
+
+        obj['windDirection'] = {
+            'value': decode_wind_direction(forecast['ddVento'])
+        }
+        obj['windSpeed'] = {
+            'value': float(forecast['ffVento']) * 0.28
         }
-      }
-    
-    hr = get_data(forecast, 'hR')
-    if hr != None:
-      hr = hr / 100
-    else:
-      hr = None
-      
-    obj['relativeHumidity'] = {
-      'value': hr
-    }
-    obj['dateIssued'] = {
-      'value': forecast['dataUpdate'],
-      'type': 'DateTime'
-    }
-    obj['dateRetrieved'] = {
-      'type': 'DateTime',
-      'value': datetime.datetime.now(lisbon_tz).replace(microsecond=0).isoformat()
-    }
-    obj['validFrom'] =  {
-      'value': forecast['dataPrev'],
-      'type': 'DateTime'
-    }
-    obj['validTo'] =    {
-      'value': valid_to.isoformat(),
-      'type': 'DateTime'
-    }
-    obj['validity'] =   {
-      'value': obj['validFrom']['value'] + '/' + obj['validTo']['value']
-    }
-    obj['address'] = {
-      'value': {
-        'addressCountry': 'PT',
-        'addressLocality': locality
-      },
-      'type': 'PostalAddress'
-    }
-    
-    obj['windDirection'] = {
-      'value': decode_wind_direction(forecast['ddVento'])
-    }
-    obj['windSpeed'] = {
-      'value': float(forecast['ffVento']) * 0.28
-    }
-    
-    weather_type_id = str(forecast['idTipoTempo'])
-    if weather_type_id in weather_type_dict:
-      obj['weatherType'] = {
-        'value': weather_type_dict[weather_type_id]
-      }
-      
-    obj['id'] = 'Portugal' + '-' + 'WeatherForecast' + '-'  + locality + '_' + obj['validFrom']['value'] + '_' + obj['validTo']['value']
-    obj['source'] = {
-      'type': 'URL',
-      'value': 'https://www.ipma.pt'
-    }
-      
-    out.append(obj)
-  
-  return out
+
+        weather_type_id = str(forecast['idTipoTempo'])
+        if weather_type_id in weather_type_dict:
+            obj['weatherType'] = {
+                'value': weather_type_dict[weather_type_id]
+            }
+
+        obj['id'] = 'Portugal' + '-' + 'WeatherForecast' + '-' + locality + \
+            '_' + obj['validFrom']['value'] + '_' + obj['validTo']['value']
+        obj['source'] = {
+            'type': 'URL',
+            'value': 'https://www.ipma.pt'
+        }
+
+        out.append(obj)
+
+    return out
 
 
 def get_data(forecast, item):
-  value = float(forecast[item])
-  if value == -99.0:
-    value = None
-  
-  return value
+    value = float(forecast[item])
+    if value == -99.0:
+        value = None
+
+    return value
+
 
 def post_data(data):
-  for a_postal_code in data:
-    if len(data[a_postal_code]) == 0:
-      continue
-    
-    data_obj = {
-      'actionType': 'APPEND',
-      'entities': data[a_postal_code]
-    }
-    data_as_str = json.dumps(data_obj)
-  
-    headers = {
-      'Content-Type':   MIME_JSON,
-      'Content-Length': len(data_as_str),
-      'Fiware-Service': FIWARE_SERVICE,
-      'Fiware-Servicepath': FIWARE_SPATH
-    }
-  
-    logger.debug('Going to persist %s (%d) to %s', a_postal_code, len(data[a_postal_code]), orion_service)
-    
-    req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-    
-    try:
-      with contextlib.closing(urllib2.urlopen(req)) as f:
-        global persisted_entities
-        persisted_entities = persisted_entities + len(data[a_postal_code])
-        logger.debug('Entities successfully created for postal code: %s', a_postal_code)
-    except urllib2.URLError as e:
-      logger.error('Error!!! %s', a_postal_code)
-      global in_error_entities
-      logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-      logger.debug('Data which failed: %s', data_as_str)
-      in_error_entities = in_error_entities + 1
+    for a_postal_code in data:
+        if len(data[a_postal_code]) == 0:
+            continue
+
+        data_obj = {
+            'actionType': 'APPEND',
+            'entities': data[a_postal_code]
+        }
+        data_as_str = json.dumps(data_obj)
+
+        headers = {
+            'Content-Type': MIME_JSON,
+            'Content-Length': len(data_as_str),
+            'Fiware-Service': FIWARE_SERVICE,
+            'Fiware-Servicepath': FIWARE_SPATH
+        }
+
+        logger.debug(
+            'Going to persist %s (%d) to %s', a_postal_code, len(
+                data[a_postal_code]), orion_service)
+
+        req = urllib2.Request(
+            url=(
+                orion_service +
+                '/v2/op/update'),
+            data=data_as_str,
+            headers=headers)
+
+        try:
+            with contextlib.closing(urllib2.urlopen(req)) as f:
+                global persisted_entities
+                persisted_entities = persisted_entities + \
+                    len(data[a_postal_code])
+                logger.debug(
+                    'Entities successfully created for postal code: %s',
+                    a_postal_code)
+        except urllib2.URLError as e:
+            logger.error('Error!!! %s', a_postal_code)
+            global in_error_entities
+            logger.error(
+                'Error while POSTing data to Orion: %d %s',
+                e.code,
+                e.read())
+            logger.debug('Data which failed: %s', data_as_str)
+            in_error_entities = in_error_entities + 1
 
 
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_weather_forecast_portugal.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('WeatherForecast')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-     
+    global logger
+
+    LOG_FILENAME = 'harvest_weather_forecast_portugal.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('WeatherForecast')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####') 
-  logger.debug('Number of localities known: %d', len(iptma_codes.keys()))
-  
-  data = get_weather_forecasted()
-  
-  post_data(data)
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of entities in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
+    setup_logger()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug('Number of localities known: %d', len(iptma_codes.keys()))
+
+    data = get_weather_forecasted()
+
+    post_data(data)
+
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug('Number of entities in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Weather/WeatherForecast/harvest/spain_weather_forecast_harvest.py b/Weather/WeatherForecast/harvest/spain_weather_forecast_harvest.py
index ba444db..c242c82 100644
--- a/Weather/WeatherForecast/harvest/spain_weather_forecast_harvest.py
+++ b/Weather/WeatherForecast/harvest/spain_weather_forecast_harvest.py
@@ -21,81 +21,84 @@
 
 MIME_JSON = 'application/json'
 FIWARE_SERVICE = 'Weather'
-FIWARE_SPATH =   '/Spain'
+FIWARE_SPATH = '/Spain'
 
 # Orion service that will store the data
 orion_service = 'http://localhost:1030'
 
 postal_codes = {
-  '47001': '47186',
-  '28001': '28079',
-  '39001': '39075',
-  '34001': '34120',
-  '34200': '34023',
-  '05194': '05123',
-  '33300': '33076',
-  '41001': '41091',
-  '46005': '46250',
-  '08001': '08019',
-  '50001': '50297',
-  '38001': '38038',
-  '35001': '35016',
-  '29001': '29067',
-  '07001': '07024',
-  '15001' : '15030'
+    '47001': '47186',
+    '28001': '28079',
+    '39001': '39075',
+    '34001': '34120',
+    '34200': '34023',
+    '05194': '05123',
+    '33300': '33076',
+    '41001': '41091',
+    '46005': '46250',
+    '08001': '08019',
+    '50001': '50297',
+    '38001': '38038',
+    '35001': '35016',
+    '29001': '29067',
+    '07001': '07024',
+    '15001': '15030'
 }
 
 localities = {
-  'Valladolid':                 '47186',
-  'Madrid':                     '28079',
-  'Santander':                  '39075',
-  'Palencia':                   '34120',
-  u'Venta de Ba帽os':            '34023',
-  'Mediana de Voltoya':         '05123',
-  'Villaviciosa':               '33076',
-  'Sevilla':                    '41091',
-  'Valencia':                   '46250',
-  'Barcelona':                  '08019',
-  'Zaragoza':                   '50297',
-  'Santa Cruz de Tenerife':     '38038',
-  'Las Palmas de Gran Canaria': '35016',
-  'Malaga':                     '29067',
-  'Formentera':                 '07024',
-  u'Coru帽a, A':                 '15030'
+    'Valladolid': '47186',
+    'Madrid': '28079',
+    'Santander': '39075',
+    'Palencia': '34120',
+    u'Venta de Ba帽os': '34023',
+    'Mediana de Voltoya': '05123',
+    'Villaviciosa': '33076',
+    'Sevilla': '41091',
+    'Valencia': '46250',
+    'Barcelona': '08019',
+    'Zaragoza': '50297',
+    'Santa Cruz de Tenerife': '38038',
+    'Las Palmas de Gran Canaria': '35016',
+    'Malaga': '29067',
+    'Formentera': '07024',
+    u'Coru帽a, A': '15030'
 }
 
 # Statistics for tracking purposes
 persisted_entities = 0
 in_error_entities = 0
 
-aemet_service    = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+aemet_service = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+
 
 def decode_wind_direction(direction):
-  dictionary = {
-    'Norte': 180,
-    'Sur': 0,
-    'Este': -90,
-    'Oeste': 90,
-    'Nordeste': -45,
-    'Noroeste': 45,
-    'Sureste': -135,
-    'Suroeste': 135,
-    'N': 180,
-    'S': 0,
-    'E': -90,
-    'O': 90,
-    'NE': -45,
-    'NO': 45,
-    'SE': -135,
-    'SO': 135
-  }
-  
-  if direction in dictionary: 
-    return dictionary[direction]
-  else: return None
-  
+    dictionary = {
+        'Norte': 180,
+        'Sur': 0,
+        'Este': -90,
+        'Oeste': 90,
+        'Nordeste': -45,
+        'Noroeste': 45,
+        'Sureste': -135,
+        'Suroeste': 135,
+        'N': 180,
+        'S': 0,
+        'E': -90,
+        'O': 90,
+        'NE': -45,
+        'NO': 45,
+        'SE': -135,
+        'SO': 135
+    }
+
+    if direction in dictionary:
+        return dictionary[direction]
+    else:
+        return None
+
 ######
 
+
 '''
 GSMA enumerated values for weatherType
 
@@ -110,7 +113,7 @@ def decode_wind_direction(direction):
 lightRain,
 heavyRainShower,
 heavyRain,
-sleetShower,    
+sleetShower,
 sleet,         --> agua nieve
 hailShower,
 hail,          --> granizo
@@ -123,332 +126,370 @@ def decode_wind_direction(direction):
 thunder
 '''
 
+
 def decode_weather_type(weather_type):
-  if weather_type == None:
-    return None
-  
-  param = weather_type.lower()
-  
-  trailing = ''
-  if param.endswith('noche'):
-    trailing = ', night'
-    param = param[0:param.index('noche')].strip()
-  
-  dictionary = {
-    'despejado': 'sunnyDay',
-    'poco nuboso': 'slightlyCloudy',
-    'intervalos nubosos': 'partlyCloudy',
-    'nuboso': 'cloudy',
-    'muy nuboso': 'veryCloudy',
-    'cubierto': 'overcast',
-    'nubes altas': 'highClouds',
-    'intervalos nubosos con lluvia escasa': 'partlyCloudy,drizzle',
-    'nuboso con lluvia escasa': 'cloudy, drizzle',
-    'muy nuboso con lluvia escasa': 'veryCloudy, drizzle',
-    'cubierto con lluvia escasa': 'overcast, drizzle',
-    'intervalos nubosos con lluvia': 'partlyCloudy,lightRain',
-    'nuboso con lluvia': 'cloudy,lightRain',
-    'muy nuboso con lluvia': 'veryCloudy, lightRain',
-    'cubierto con lluvia': 'overcast, lightRain',
-    'intervalos nubosos con nieve escasa': 'partlyCloudy, lightSnow',
-    'nuboso con nieve escasa': 'cloudy, lightSnow',
-    'muy nuboso con nieve escasa': 'veryCloudy, lightSnow',
-    'cubierto con nieve escasa': 'overcast, lightSnow',
-    'intervalos nubosos con nieve': 'partlyCloudy,snow',
-    'nuboso con nieve': 'cloudy, snow',
-    'muy nuboso con nieve': 'veryCloudy, snow',
-    'cubierto con nieve': 'overcast, snow',
-    'intervalos nubosos con tormenta': 'partlyCloudy, thunder',
-    'nuboso con tormenta': 'cloudy, thunder',
-    'muy nuboso con tormenta': 'veryCloudy,thunder',
-    'cubierto con tormenta': 'overcast, thunder',
-    'intervalos nubosos con tormenta y lluvia escasa': 'partlyCloudy, thunder, lightRainShower',
-    'nuboso con tormenta y lluvia escasa': 'cloudy, thunder, lightRainShower',
-    'muy nuboso con tormenta y lluvia escasa': 'veryCloudy, thunder, lightRainShower',
-    'cubierto con tormenta y lluvia escasa': 'overcast, thunder, lightRainShower',
-    'despejado noche': 'clearNight'
-  }
-  
-  if param in dictionary:
-    out = dictionary[param] + trailing
-  else: out = None
-  
-  return out
+    if weather_type is None:
+        return None
+
+    param = weather_type.lower()
+
+    trailing = ''
+    if param.endswith('noche'):
+        trailing = ', night'
+        param = param[0:param.index('noche')].strip()
+
+    dictionary = {
+        'despejado': 'sunnyDay',
+        'poco nuboso': 'slightlyCloudy',
+        'intervalos nubosos': 'partlyCloudy',
+        'nuboso': 'cloudy',
+        'muy nuboso': 'veryCloudy',
+        'cubierto': 'overcast',
+        'nubes altas': 'highClouds',
+        'intervalos nubosos con lluvia escasa': 'partlyCloudy,drizzle',
+        'nuboso con lluvia escasa': 'cloudy, drizzle',
+        'muy nuboso con lluvia escasa': 'veryCloudy, drizzle',
+        'cubierto con lluvia escasa': 'overcast, drizzle',
+        'intervalos nubosos con lluvia': 'partlyCloudy,lightRain',
+        'nuboso con lluvia': 'cloudy,lightRain',
+        'muy nuboso con lluvia': 'veryCloudy, lightRain',
+        'cubierto con lluvia': 'overcast, lightRain',
+        'intervalos nubosos con nieve escasa': 'partlyCloudy, lightSnow',
+        'nuboso con nieve escasa': 'cloudy, lightSnow',
+        'muy nuboso con nieve escasa': 'veryCloudy, lightSnow',
+        'cubierto con nieve escasa': 'overcast, lightSnow',
+        'intervalos nubosos con nieve': 'partlyCloudy,snow',
+        'nuboso con nieve': 'cloudy, snow',
+        'muy nuboso con nieve': 'veryCloudy, snow',
+        'cubierto con nieve': 'overcast, snow',
+        'intervalos nubosos con tormenta': 'partlyCloudy, thunder',
+        'nuboso con tormenta': 'cloudy, thunder',
+        'muy nuboso con tormenta': 'veryCloudy,thunder',
+        'cubierto con tormenta': 'overcast, thunder',
+        'intervalos nubosos con tormenta y lluvia escasa': 'partlyCloudy, thunder, lightRainShower',
+        'nuboso con tormenta y lluvia escasa': 'cloudy, thunder, lightRainShower',
+        'muy nuboso con tormenta y lluvia escasa': 'veryCloudy, thunder, lightRainShower',
+        'cubierto con tormenta y lluvia escasa': 'overcast, thunder, lightRainShower',
+        'despejado noche': 'clearNight'}
+
+    if param in dictionary:
+        out = dictionary[param] + trailing
+    else:
+        out = None
+
+    return out
 
 #########
-      
+
+
 def get_weather_forecasted():
-  # Indexed by postal code
-  out = { }
-  for postal_code in postal_codes:
-    out[postal_code] = []
-    
-    param = postal_codes[postal_code]
-      
-    source = aemet_service.format(param)
-    req = urllib2.Request(url=source)
-    
-    try:
-      with contextlib.closing(urllib2.urlopen(req)) as f:
-        xml_data = f.read()
-        
-        logger.debug('All AEMET data read for %s', postal_code)
-        
-        DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
-        
-        address_locality = DOMTree.getElementsByTagName('nombre')[0].firstChild.nodeValue
-        address = { }
-        address['addressCountry'] = country
-        address['postalCode'] = postal_code
-        address['addressLocality'] = address_locality
-        
-        created =  DOMTree.getElementsByTagName('elaborado')[0].firstChild.nodeValue
-        
-        forecasts = DOMTree.getElementsByTagName('prediccion')[0].getElementsByTagName('dia')
-        
-        for forecast in forecasts:
-          date = forecast.getAttribute('fecha')
-          normalizedForecast = parse_aemet_forecast(forecast, date)
-          counter = 1
-          for f in normalizedForecast:
-            f['type'] = 'WeatherForecast'
-            f['id'] = generate_id(postal_code, country, f['validity']['value'])
-            f['address'] = {
-              'value': address,
-              'type':  'PostalAddress'
-            }
-            f['dateIssued'] = {
-              'type': 'DateTime',
-              'value': parser.parse(created).replace(tzinfo=madrid_tz).isoformat()
-            }
-            f['dateRetrieved'] = {
-              'type': 'DateTime',
-              'value': datetime.datetime.now(madrid_tz).replace(microsecond=0).isoformat()
-            }
-            f['source'] = {
-              'value': source,
-              'type': 'URL'
-            }
-            f['dataProvider'] = {
-              'value': 'TEF'
-            }
-            counter += 1
-            out[postal_code].append(f)
-    except urllib2.URLError as e:
-      logger.error('Error while retrieving AEMET data for: %s. HTTP Error: %d', postal_code, e.code)
-  
-  return out
-    
+    # Indexed by postal code
+    out = {}
+    for postal_code in postal_codes:
+        out[postal_code] = []
+
+        param = postal_codes[postal_code]
+
+        source = aemet_service.format(param)
+        req = urllib2.Request(url=source)
+
+        try:
+            with contextlib.closing(urllib2.urlopen(req)) as f:
+                xml_data = f.read()
+
+                logger.debug('All AEMET data read for %s', postal_code)
+
+                DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
+
+                address_locality = DOMTree.getElementsByTagName(
+                    'nombre')[0].firstChild.nodeValue
+                address = {}
+                address['addressCountry'] = country
+                address['postalCode'] = postal_code
+                address['addressLocality'] = address_locality
+
+                created = DOMTree.getElementsByTagName(
+                    'elaborado')[0].firstChild.nodeValue
+
+                forecasts = DOMTree.getElementsByTagName(
+                    'prediccion')[0].getElementsByTagName('dia')
+
+                for forecast in forecasts:
+                    date = forecast.getAttribute('fecha')
+                    normalizedForecast = parse_aemet_forecast(forecast, date)
+                    counter = 1
+                    for f in normalizedForecast:
+                        f['type'] = 'WeatherForecast'
+                        f['id'] = generate_id(
+                            postal_code, country, f['validity']['value'])
+                        f['address'] = {
+                            'value': address,
+                            'type': 'PostalAddress'
+                        }
+                        f['dateIssued'] = {
+                            'type': 'DateTime',
+                            'value': parser.parse(created).replace(
+                                tzinfo=madrid_tz).isoformat()}
+                        f['dateRetrieved'] = {
+                            'type': 'DateTime',
+                            'value': datetime.datetime.now(madrid_tz).replace(
+                                microsecond=0).isoformat()}
+                        f['source'] = {
+                            'value': source,
+                            'type': 'URL'
+                        }
+                        f['dataProvider'] = {
+                            'value': 'TEF'
+                        }
+                        counter += 1
+                        out[postal_code].append(f)
+        except urllib2.URLError as e:
+            logger.error(
+                'Error while retrieving AEMET data for: %s. HTTP Error: %d',
+                postal_code,
+                e.code)
+
+    return out
+
 
 def parse_aemet_forecast(forecast, date):
-  periods = { }
-  out = []
-  
-  parsed_date = parser.parse(date)
-  
-  pops = forecast.getElementsByTagName('prob_precipitacion')
-  for pop in pops:
-    period = pop.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if pop.firstChild and pop.firstChild.nodeValue:
-      insert_into_period(periods, period,
-                  'precipitationProbability', float(pop.firstChild.nodeValue) / 100.0)
-  
-  period = None
-  weather_types = forecast.getElementsByTagName('estado_cielo')
-  for weather_type in weather_types:
-    period = weather_type.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if weather_type.firstChild and weather_type.firstChild.nodeValue:
-      insert_into_period(periods, period, 'weatherType',
-                         decode_weather_type(weather_type.getAttribute('descripcion')))
-      insert_into_period(periods, period, 'weatherType_ES', weather_type.getAttribute('descripcion'))
-  
-  period = None
-  wind_data = forecast.getElementsByTagName('viento')
-  for wind in wind_data:
-    period = wind.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    wind_direction = wind.getElementsByTagName('direccion')[0]
-    wind_speed = wind.getElementsByTagName('velocidad')[0]
-    if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windSpeed',
-                         round(float(wind_speed.firstChild.nodeValue) * 0.28, 2))
-    if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windDirection',
-                         decode_wind_direction(wind_direction.firstChild.nodeValue))
-  
-  temperature_node = forecast.getElementsByTagName('temperatura')[0]
-  max_temp = float(temperature_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp = float(temperature_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temperature_node, periods, 'temperature')
-  
-  temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
-  max_temp_feels = float(temp_feels_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp_feels = float(temp_feels_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
-  
-  humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
-  max_humidity = float(humidity_node.getElementsByTagName('maxima')[0].firstChild.nodeValue) / 100.0
-  min_humidity = float(humidity_node.getElementsByTagName('minima')[0].firstChild.nodeValue) / 100.0
-  get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
-  
-  for period in periods:
-    period_items = period.split('-')
-    period_start = period_items[0]
-    period_end = period_items[1]
-    end_hour = int(period_end)
-    end_date = copy.deepcopy(parsed_date)
-    if end_hour > 23:
-      end_hour = 0
-      end_date = parsed_date + datetime.timedelta(days=1)
-    
-    start_date = parsed_date.replace(hour=int(period_start), minute=0, second=0)
-    end_date = end_date.replace(hour=end_hour,minute=0,second=0)   
-    
-    objPeriod = periods[period]
-  
-    valid_from = start_date.replace(tzinfo=madrid_tz).isoformat()
-    valid_to = end_date.replace(tzinfo=madrid_tz).isoformat()
-    
-    objPeriod['validity'] = {
-      'value': valid_from + '/' + valid_to,
-      'type': 'Text'
-    }
-    
-    # Custom fields as Orion Context Broker does not support ISO8601 intervals
-    objPeriod['validFrom'] = {
-      'value': valid_from,
-      'type': 'DateTime'
-    }
-    objPeriod['validTo']  = {
-      'value': valid_to,
-      'type': 'DateTime'
-    }
-    
-    maximum = { }
-    objPeriod['dayMaximum'] = {
-      'value': maximum
-    }
-    minimum = { }
-    objPeriod['dayMinimum'] = {
-      'value': minimum
-    }
-    
-    maximum['temperature'] = max_temp
-    minimum['temperature'] = min_temp
-    
-    maximum['relativeHumidity'] = max_humidity
-    minimum['relativeHumidity'] = min_humidity
-    
-    maximum['feelsLikeTemperature'] = max_temp_feels
-    minimum['feelsLikeTemperature'] = min_temp_feels
-    
-    out.append(objPeriod)
-  
-  return out
+    periods = {}
+    out = []
+
+    parsed_date = parser.parse(date)
+
+    pops = forecast.getElementsByTagName('prob_precipitacion')
+    for pop in pops:
+        period = pop.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if pop.firstChild and pop.firstChild.nodeValue:
+            insert_into_period(
+                periods, period, 'precipitationProbability', float(
+                    pop.firstChild.nodeValue) / 100.0)
+
+    period = None
+    weather_types = forecast.getElementsByTagName('estado_cielo')
+    for weather_type in weather_types:
+        period = weather_type.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if weather_type.firstChild and weather_type.firstChild.nodeValue:
+            insert_into_period(
+                periods, period, 'weatherType', decode_weather_type(
+                    weather_type.getAttribute('descripcion')))
+            insert_into_period(
+                periods,
+                period,
+                'weatherType_ES',
+                weather_type.getAttribute('descripcion'))
+
+    period = None
+    wind_data = forecast.getElementsByTagName('viento')
+    for wind in wind_data:
+        period = wind.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        wind_direction = wind.getElementsByTagName('direccion')[0]
+        wind_speed = wind.getElementsByTagName('velocidad')[0]
+        if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
+            insert_into_period(periods, period, 'windSpeed', round(
+                float(wind_speed.firstChild.nodeValue) * 0.28, 2))
+        if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
+            insert_into_period(
+                periods, period, 'windDirection', decode_wind_direction(
+                    wind_direction.firstChild.nodeValue))
+
+    temperature_node = forecast.getElementsByTagName('temperatura')[0]
+    max_temp = float(temperature_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp = float(temperature_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temperature_node, periods, 'temperature')
+
+    temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
+    max_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
+
+    humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
+    max_humidity = float(humidity_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue) / 100.0
+    min_humidity = float(humidity_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue) / 100.0
+    get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
+
+    for period in periods:
+        period_items = period.split('-')
+        period_start = period_items[0]
+        period_end = period_items[1]
+        end_hour = int(period_end)
+        end_date = copy.deepcopy(parsed_date)
+        if end_hour > 23:
+            end_hour = 0
+            end_date = parsed_date + datetime.timedelta(days=1)
+
+        start_date = parsed_date.replace(
+            hour=int(period_start), minute=0, second=0)
+        end_date = end_date.replace(hour=end_hour, minute=0, second=0)
+
+        objPeriod = periods[period]
+
+        valid_from = start_date.replace(tzinfo=madrid_tz).isoformat()
+        valid_to = end_date.replace(tzinfo=madrid_tz).isoformat()
+
+        objPeriod['validity'] = {
+            'value': valid_from + '/' + valid_to,
+            'type': 'Text'
+        }
+
+        # Custom fields as Orion Context Broker does not support ISO8601
+        # intervals
+        objPeriod['validFrom'] = {
+            'value': valid_from,
+            'type': 'DateTime'
+        }
+        objPeriod['validTo'] = {
+            'value': valid_to,
+            'type': 'DateTime'
+        }
+
+        maximum = {}
+        objPeriod['dayMaximum'] = {
+            'value': maximum
+        }
+        minimum = {}
+        objPeriod['dayMinimum'] = {
+            'value': minimum
+        }
+
+        maximum['temperature'] = max_temp
+        minimum['temperature'] = min_temp
+
+        maximum['relativeHumidity'] = max_humidity
+        minimum['relativeHumidity'] = min_humidity
+
+        maximum['feelsLikeTemperature'] = max_temp_feels
+        minimum['feelsLikeTemperature'] = min_temp_feels
+
+        out.append(objPeriod)
+
+    return out
 
 
 def get_parameter_data(node, periods, parameter, factor=1.0):
-  param_periods = node.getElementsByTagName('dato')
-  for param in param_periods:
-    hour_str = param.getAttribute('hora')
-    hour = int(hour_str)
-    interval_start = hour - 6
-    interval_start_str = str(interval_start)
-    if interval_start < 10:
-      interval_start_str = '0' + str(interval_start)
-      
-    period = interval_start_str + '-' + hour_str
-    if param.firstChild and param.firstChild.nodeValue:
-      param_val = float(param.firstChild.nodeValue)
-      insert_into_period(periods, period, parameter, param_val / factor)
+    param_periods = node.getElementsByTagName('dato')
+    for param in param_periods:
+        hour_str = param.getAttribute('hora')
+        hour = int(hour_str)
+        interval_start = hour - 6
+        interval_start_str = str(interval_start)
+        if interval_start < 10:
+            interval_start_str = '0' + str(interval_start)
+
+        period = interval_start_str + '-' + hour_str
+        if param.firstChild and param.firstChild.nodeValue:
+            param_val = float(param.firstChild.nodeValue)
+            insert_into_period(periods, period, parameter, param_val / factor)
 
 
 def insert_into_period(periods, period, attribute, value):
-  if not period in periods:
-    periods[period] = { }
-  
-  periods[period][attribute] = {
-    'value': value
-  }
+    if period not in periods:
+        periods[period] = {}
+
+    periods[period][attribute] = {
+        'value': value
+    }
+
 
 def generate_id(postal_code, country, validity):
-  # Remove timezone info from the validity string
-  elements = validity.split('/')
-  period_info = elements[0][0:elements[0].index('+')] + '_' + elements[1][0:elements[1].index('+')]
-  
-  return country + '-' + 'WeatherForecast' + '-' + postal_code + '_' + period_info
+    # Remove timezone info from the validity string
+    elements = validity.split('/')
+    period_info = elements[0][0:elements[0].index(
+        '+')] + '_' + elements[1][0:elements[1].index('+')]
+
+    return country + '-' + 'WeatherForecast' + '-' + postal_code + '_' + period_info
 
 
 def post_data(data):
-  for a_postal_code in data:
-    if len(data[a_postal_code]) == 0:
-      continue
-    
-    data_obj = {
-      'actionType': 'APPEND',
-      'entities': data[a_postal_code]
-    }
-    data_as_str = json.dumps(data_obj)
-  
-    headers = {
-      'Content-Type':   MIME_JSON,
-      'Content-Length': len(data_as_str),
-      'Fiware-Service': FIWARE_SERVICE,
-      'Fiware-Servicepath': FIWARE_SPATH
-    }
-  
-    logger.debug('Going to persist %s (%d) to %s', a_postal_code, len(data[a_postal_code]), orion_service)
-    
-    req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-    
-    try:
-      with contextlib.closing(urllib2.urlopen(req)) as f:
-        global persisted_entities
-        persisted_entities = persisted_entities + len(data[a_postal_code])
-        logger.debug('Entities successfully created for postal code: %s', a_postal_code)
-    except urllib2.URLError as e:
-      logger.error('Error!!! %s', a_postal_code)
-      global in_error_entities
-      logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-      logger.debug('Data which failed: %s', data_as_str)
-      in_error_entities = in_error_entities + 1
-      
+    for a_postal_code in data:
+        if len(data[a_postal_code]) == 0:
+            continue
+
+        data_obj = {
+            'actionType': 'APPEND',
+            'entities': data[a_postal_code]
+        }
+        data_as_str = json.dumps(data_obj)
+
+        headers = {
+            'Content-Type': MIME_JSON,
+            'Content-Length': len(data_as_str),
+            'Fiware-Service': FIWARE_SERVICE,
+            'Fiware-Servicepath': FIWARE_SPATH
+        }
+
+        logger.debug(
+            'Going to persist %s (%d) to %s', a_postal_code, len(
+                data[a_postal_code]), orion_service)
+
+        req = urllib2.Request(
+            url=(
+                orion_service +
+                '/v2/op/update'),
+            data=data_as_str,
+            headers=headers)
+
+        try:
+            with contextlib.closing(urllib2.urlopen(req)) as f:
+                global persisted_entities
+                persisted_entities = persisted_entities + \
+                    len(data[a_postal_code])
+                logger.debug(
+                    'Entities successfully created for postal code: %s',
+                    a_postal_code)
+        except urllib2.URLError as e:
+            logger.error('Error!!! %s', a_postal_code)
+            global in_error_entities
+            logger.error(
+                'Error while POSTing data to Orion: %d %s',
+                e.code,
+                e.read())
+            logger.debug('Data which failed: %s', data_as_str)
+            in_error_entities = in_error_entities + 1
 
 
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_weather_forecast_spain.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('WeatherForecast')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-     
+    global logger
+
+    LOG_FILENAME = 'harvest_weather_forecast_spain.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('WeatherForecast')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####') 
-  logger.debug('Number of localities known: %d', len(postal_codes.keys()))
-  
-  data = get_weather_forecasted()
-  
-  post_data(data)
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of entities in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
+    setup_logger()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug('Number of localities known: %d', len(postal_codes.keys()))
+
+    data = get_weather_forecasted()
 
+    post_data(data)
 
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug('Number of entities in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Weather/WeatherObserved/harvest/portugal_weather_observed_harvest.py b/Weather/WeatherObserved/harvest/portugal_weather_observed_harvest.py
index 0245109..b17a2d7 100644
--- a/Weather/WeatherObserved/harvest/portugal_weather_observed_harvest.py
+++ b/Weather/WeatherObserved/harvest/portugal_weather_observed_harvest.py
@@ -14,7 +14,7 @@
 import re
 
 # List of known weather stations
-station_data = { }
+station_data = {}
 
 # Orion service that will store the data
 orion_service = 'http://localhost:1030'
@@ -29,201 +29,224 @@
 
 MIME_JSON = 'application/json'
 FIWARE_SERVICE = 'Weather'
-FIWARE_SPATH =   '/Portugal'
+FIWARE_SPATH = '/Portugal'
+
 
 def decode_wind_direction(direction):
-  dictionary = {
-    '9': 180,  # North
-    '5': 0,    # South
-    '3': -90,  # East
-    '7': 90,   # West
-    '2': -135, # Northeast 
-    '8': 135,  # Northwest
-    '4': -45,  # Southeast
-    '6': 45    # Southwest     
-  }
-  
-  if direction in dictionary: 
-    return dictionary[direction]
-  else: return None
+    dictionary = {
+        '9': 180,  # North
+        '5': 0,    # South
+        '3': -90,  # East
+        '7': 90,   # West
+        '2': -135,  # Northeast
+        '8': 135,  # Northwest
+        '4': -45,  # Southeast
+        '6': 45    # Southwest
+    }
+
+    if direction in dictionary:
+        return dictionary[direction]
+    else:
+        return None
 
 
 # Sanitize string to avoid forbidden characters by Orion
 def sanitize(str_in):
-  return re.sub(r"[<(>)\"\'=;-]", "", str_in)
+    return re.sub(r"[<(>)\"\'=;-]", "", str_in)
+
 
 def get_weather_observed_portugal():
-  req = urllib2.Request(url='http://www.ipma.pt/resources.www/transf/obs-sup/observations.json')
-  with contextlib.closing(urllib2.urlopen(req)) as f:
-    json_str = f.read()
-    
-    source_data = json.loads(json_str)
-    
-    # Contains observation data indexed by station code
-    observation_data = { }
-    
-    for date in source_data:
-      for station_code in source_data[date]:
-        if station_code not in observation_data:
-          observation_data[station_code] = []
-    
-        this_station_data = source_data[date][station_code]
-        if this_station_data is None:
-          continue
-          
-        observation = {
-          'type': 'WeatherObserved',
-          'stationCode': {
-            'value': station_code
-          },
-          'stationName': {
-            'value': sanitize(station_data[station_code]['name'])
-          }
-        }
-        
-        observation['temperature'] = {
-          'value': get_value(this_station_data['temperatura'])
-        }
-        observation['windSpeed'] = {
-          'value': get_value(this_station_data['intensidadeVento'])
-        }
-        observation['windDirection'] = {
-          'value': decode_wind_direction(str(this_station_data['idDireccVento']))
-        }
-        observation['precipitation'] = {
-          'value': get_value(this_station_data['precAcumulada'])
-        }
-        
-        observation['atmosfericPressure'] = {
-          'value': get_value(this_station_data['pressao'])
-        }
-          
-        observation['relativeHumidity'] = {
-          'value': get_value(this_station_data['humidade'], 100)
-        }
-        
-        observation['dateObserved'] = {
-          'value': date,
-          'type': 'DateTime'
-        }
-        
-        observation['source'] = {
-          'value': 'https://www.ipma.pt/',
-          'type': 'URL'
-        }
-        observation['dataProvider'] = {
-          'value': 'TEF'
-        }
-        observation['location'] = {
-          'value': station_data[station_code]['location'],
-          'type': 'geo:json'
-        }
-        
-        observation['id'] = 'Portugal-WeatherObserved' + '-' + station_code + '-' + date
-        
-        observation_data[station_code].append(observation)
-
-        # A batch of station data is persisted      
-        # 
-  for station_code in observation_data:
-    if len(observation_data[station_code]) > 0:
-      latest_observation = observation_data[station_code][-1]
-      latest_observation['id'] = 'Portugal-WeatherObserved' + '-' + station_code + '-' + 'latest'
-      
-    post_station_data_batch(station_code, observation_data[station_code])
+    req = urllib2.Request(
+        url='http://www.ipma.pt/resources.www/transf/obs-sup/observations.json')
+    with contextlib.closing(urllib2.urlopen(req)) as f:
+        json_str = f.read()
+
+        source_data = json.loads(json_str)
+
+        # Contains observation data indexed by station code
+        observation_data = {}
+
+        for date in source_data:
+            for station_code in source_data[date]:
+                if station_code not in observation_data:
+                    observation_data[station_code] = []
+
+                this_station_data = source_data[date][station_code]
+                if this_station_data is None:
+                    continue
+
+                observation = {
+                    'type': 'WeatherObserved',
+                    'stationCode': {
+                        'value': station_code
+                    },
+                    'stationName': {
+                        'value': sanitize(station_data[station_code]['name'])
+                    }
+                }
+
+                observation['temperature'] = {
+                    'value': get_value(this_station_data['temperatura'])
+                }
+                observation['windSpeed'] = {
+                    'value': get_value(this_station_data['intensidadeVento'])
+                }
+                observation['windDirection'] = {'value': decode_wind_direction(
+                    str(this_station_data['idDireccVento']))}
+                observation['precipitation'] = {
+                    'value': get_value(this_station_data['precAcumulada'])
+                }
+
+                observation['atmosfericPressure'] = {
+                    'value': get_value(this_station_data['pressao'])
+                }
+
+                observation['relativeHumidity'] = {
+                    'value': get_value(this_station_data['humidade'], 100)
+                }
+
+                observation['dateObserved'] = {
+                    'value': date,
+                    'type': 'DateTime'
+                }
+
+                observation['source'] = {
+                    'value': 'https://www.ipma.pt/',
+                    'type': 'URL'
+                }
+                observation['dataProvider'] = {
+                    'value': 'TEF'
+                }
+                observation['location'] = {
+                    'value': station_data[station_code]['location'],
+                    'type': 'geo:json'
+                }
+
+                observation['id'] = 'Portugal-WeatherObserved' + \
+                    '-' + station_code + '-' + date
+
+                observation_data[station_code].append(observation)
+
+                # A batch of station data is persisted
+                #
+    for station_code in observation_data:
+        if len(observation_data[station_code]) > 0:
+            latest_observation = observation_data[station_code][-1]
+            latest_observation['id'] = 'Portugal-WeatherObserved' + \
+                '-' + station_code + '-' + 'latest'
+
+        post_station_data_batch(station_code, observation_data[station_code])
+
 
 def get_value(value, scale=1):
-  out = value / scale
-  if value < 0:
-    out = None
-  
-  return out
+    out = value / scale
+    if value < 0:
+        out = None
+
+    return out
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
+
+
 def post_station_data_batch(station_code, data):
-  if len(data) == 0:
-    return
-  
-  data_obj = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  data_as_str = json.dumps(data_obj)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SPATH
-  }
-  
-  logger.debug('Going to persist %s (%d) to %s', station_code, len(data), orion_service)
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-  
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      global persisted_stations
-      persisted_entities = persisted_entities + len(data)
-      persisted_stations += 1
-      logger.debug('Entities successfully created for station: %s %d/%d',
-                   station_code, persisted_stations, total_stations)
-  except urllib2.URLError as e:
-    logger.error('Error!!! %s', station_code)
-    global in_error_entities
-    logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-    logger.debug('Data which failed: %s', data_as_str)
-    in_error_entities = in_error_entities + 1
-    
-    
-  
+    if len(data) == 0:
+        return
+
+    data_obj = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+    data_as_str = json.dumps(data_obj)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SPATH
+    }
+
+    logger.debug(
+        'Going to persist %s (%d) to %s',
+        station_code,
+        len(data),
+        orion_service)
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            global persisted_stations
+            persisted_entities = persisted_entities + len(data)
+            persisted_stations += 1
+            logger.debug('Entities successfully created for station: %s %d/%d',
+                         station_code, persisted_stations, total_stations)
+    except urllib2.URLError as e:
+        logger.error('Error!!! %s', station_code)
+        global in_error_entities
+        logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+        logger.debug('Data which failed: %s', data_as_str)
+        in_error_entities = in_error_entities + 1
+
+
 # Reads station data from CSV file
 def load_station_data():
-  req = urllib2.Request(url='http://www.ipma.pt/resources.www/transf/obs-sup/stations.json')
-  
-  with contextlib.closing(urllib2.urlopen(req)) as f:
-    json_str = f.read()
-    data = json.loads(json_str)
-    
-    for station in data:
-      station_code = str(station['properties']['idEstacao'])
-      
-      station_data[station_code] = {
-        'name': sanitize(station['properties']['localEstacao']),
-        'location': station['geometry']
-      }
-       
-      
+    req = urllib2.Request(
+        url='http://www.ipma.pt/resources.www/transf/obs-sup/stations.json')
+
+    with contextlib.closing(urllib2.urlopen(req)) as f:
+        json_str = f.read()
+        data = json.loads(json_str)
+
+        for station in data:
+            station_code = str(station['properties']['idEstacao'])
+
+            station_data[station_code] = {
+                'name': sanitize(station['properties']['localEstacao']),
+                'location': station['geometry']
+            }
+
+
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_weather_observed_portugal.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('WeatherObserved')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-     
+    global logger
+
+    LOG_FILENAME = 'harvest_weather_observed_portugal.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('WeatherObserved')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  load_station_data()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####') 
-  logger.debug('Number of weather stations known: %d', len(station_data.keys()))
-  total_stations = len(station_data.keys())
-  
-  get_weather_observed_portugal()
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of stations in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
-  
\ No newline at end of file
+    setup_logger()
+
+    load_station_data()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug(
+        'Number of weather stations known: %d', len(
+            station_data.keys()))
+    total_stations = len(station_data.keys())
+
+    get_weather_observed_portugal()
+
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug('Number of stations in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Weather/WeatherObserved/harvest/spain_weather_observed_harvest.py b/Weather/WeatherObserved/harvest/spain_weather_observed_harvest.py
index 094506e..bc3be26 100644
--- a/Weather/WeatherObserved/harvest/spain_weather_observed_harvest.py
+++ b/Weather/WeatherObserved/harvest/spain_weather_observed_harvest.py
@@ -14,16 +14,61 @@
 import re
 
 # List of known weather stations
-station_data = { }
+station_data = {}
 
 # Orion service that will store the data
 orion_service = 'http://localhost:1030'
 
 # Stations in error so these are skipped when retrieving data
-station_code_exceptions = ['9946X', '1658', '349', '9894X', '1010X', '9198A', '6381', '9720X', 'C619Y', \
-'1178Y', '9111', '9718X', '0244X', '6268X', '3254Y', '6340X', '3386A', '3391', '7012C', '9918X', '8191Y', '5192', '76',\
-'4492E', '1331D', '0171C', '1583X','8210Y', '9814A', '9994X', '9174', '2084Y', 'C917E', '3337U', '2661', '367',\
-'9263I', '5246', '7250X', '2182C', '0229I', '1735X', 'C229X', '9531X', '1561I', '2150H', '0034X', '1036A']
+station_code_exceptions = [
+    '9946X',
+    '1658',
+    '349',
+    '9894X',
+    '1010X',
+    '9198A',
+    '6381',
+    '9720X',
+    'C619Y',
+    '1178Y',
+    '9111',
+    '9718X',
+    '0244X',
+    '6268X',
+    '3254Y',
+    '6340X',
+    '3386A',
+    '3391',
+    '7012C',
+    '9918X',
+    '8191Y',
+    '5192',
+    '76',
+    '4492E',
+    '1331D',
+    '0171C',
+    '1583X',
+    '8210Y',
+    '9814A',
+    '9994X',
+    '9174',
+    '2084Y',
+    'C917E',
+    '3337U',
+    '2661',
+    '367',
+    '9263I',
+    '5246',
+    '7250X',
+    '2182C',
+    '0229I',
+    '1735X',
+    'C229X',
+    '9531X',
+    '1561I',
+    '2150H',
+    '0034X',
+    '1036A']
 
 logger = None
 
@@ -40,240 +85,261 @@
 
 MIME_JSON = 'application/json'
 FIWARE_SERVICE = 'Weather'
-FIWARE_SPATH =   '/Spain'
+FIWARE_SPATH = '/Spain'
+
 
 def decode_wind_direction(direction):
-  dictionary = {
-    'Norte': 180,
-    'Sur': 0,
-    'Este': -90,
-    'Oeste': 90,
-    'Nordeste': -135,
-    'Noroeste': 135,
-    'Sureste': -45,
-    'Suroeste': 45     
-  }
-  
-  if direction in dictionary: 
-    return dictionary[direction]
-  else: return None
+    dictionary = {
+        'Norte': 180,
+        'Sur': 0,
+        'Este': -90,
+        'Oeste': 90,
+        'Nordeste': -135,
+        'Noroeste': 135,
+        'Sureste': -45,
+        'Suroeste': 45
+    }
+
+    if direction in dictionary:
+        return dictionary[direction]
+    else:
+        return None
 
 
 # Sanitize string to avoid forbidden characters by Orion
 def sanitize(str_in):
-  return re.sub(r"[<(>)\"\'=;-]", "", str_in)
+    return re.sub(r"[<(>)\"\'=;-]", "", str_in)
 
 
 def get_data(row, index, conversion=float, factor=1.0):
-  out = None
-  
-  value = row[index]
-  if(value != ''):
-    out = conversion(value) / factor
-    
-  return out
+    out = None
+
+    value = row[index]
+    if(value != ''):
+        out = conversion(value) / factor
+
+    return out
 
 
 def get_weather_observed_spain():
     num = 0
-    
+
     for station_code in station_data:
-      out = []
-      num += 1
-      if station_code in station_code_exceptions:
-        continue
-    
-      source = weather_observed.format(station_code, station_code)
-      
-      logger.debug('Requesting data from station: %s', station_code)
-      
-      req = urllib2.Request(url=source)
-      try: f = urllib2.urlopen(req)
-      except urllib2.URLError as e:
-        logger.error('Error while calling: %s : %s', source, e)
-        if f != None:
-          f.close()
-        continue
-    
-      csv_data = f.read()
-      
-      if csv_data.find('initial-scale') != -1:
-        logger.debug('Skipping: %s', station_code)
-        continue
-      
-      logger.debug('Data read successfully: %s', station_code)
-      
-      csv_file = StringIO.StringIO(csv_data)
-      reader = csv.reader(csv_file, delimiter=',')
-      
-      index = 0
-      for row in reader:
-        if index < 4:
-          index += 1
-          continue
-        
-        observation = {
-          'type': 'WeatherObserved',
-          'stationCode': {
-            'value': station_code
-          },
-          'stationName': {
-            'value': sanitize(station_data[station_code]['name'])
-          }
-        }
-        if len(row) < 2:
-          continue
-        
-        observation['temperature'] = {
-          'value':get_data(row, 1)
-        }
-        observation['windSpeed'] = {
-          'value': get_data(row, 2, float, 1/0.28)
-        }
-        observation['windDirection'] = {
-          'value': decode_wind_direction(row[3])
-        }
-        observation['precipitation'] = {
-          'value': get_data(row, 6)
-        }
-        observation['atmosphericPressure'] = {
-          'value': get_data(row, 7)
-        }
-        observation['pressureTendency'] =  {
-          'value': get_data(row, 8)
-        }
-        observation['relativeHumidity'] = {
-          'value': get_data(row, 9, factor=100.0)
-        }
-        
-        date_observed = datetime.datetime.strptime(row[0], '%d/%m/%Y %H:%M')
-        observation['dateObserved'] = {
-          'value': date_observed.replace(tzinfo=madrid_tz).isoformat(),
-          'type': 'DateTime'
-        }
-        observation['source'] = {
-          'value': 'http://www.aemet.es',
-          'type': 'URL'
-        }
-        observation['dataProvider'] = {
-          'value': 'TEF'
-        }
-        observation['address'] = {
-          'value': {
-            'addressLocality': sanitize(station_data[station_code]['address']),
-            'addressCountry': 'ES'
-          },
-          'type': 'PostalAddress'
-        }
-        observation['location'] = station_data[station_code]['location']
-        
-        observation['id'] = 'Spain-WeatherObserved' + '-' + station_code + '-' + date_observed.isoformat()
-        
-        out.append(observation)
-      
-      f.close()
-      
-      # Last observation is tagged as 'latest'
-      if len(out)> 0:
-        latest_observation = out[-1]
-        latest_observation['id'] = 'Spain-WeatherObserved' + '-' + station_code + '-' + 'latest'
-        
-      # A batch of station data is persisted      
-      post_station_data_batch(station_code, out)
+        out = []
+        num += 1
+        if station_code in station_code_exceptions:
+            continue
+
+        source = weather_observed.format(station_code, station_code)
+
+        logger.debug('Requesting data from station: %s', station_code)
+
+        req = urllib2.Request(url=source)
+        try:
+            f = urllib2.urlopen(req)
+        except urllib2.URLError as e:
+            logger.error('Error while calling: %s : %s', source, e)
+            if f is not None:
+                f.close()
+            continue
+
+        csv_data = f.read()
+
+        if csv_data.find('initial-scale') != -1:
+            logger.debug('Skipping: %s', station_code)
+            continue
+
+        logger.debug('Data read successfully: %s', station_code)
+
+        csv_file = StringIO.StringIO(csv_data)
+        reader = csv.reader(csv_file, delimiter=',')
+
+        index = 0
+        for row in reader:
+            if index < 4:
+                index += 1
+                continue
+
+            observation = {
+                'type': 'WeatherObserved',
+                'stationCode': {
+                    'value': station_code
+                },
+                'stationName': {
+                    'value': sanitize(station_data[station_code]['name'])
+                }
+            }
+            if len(row) < 2:
+                continue
+
+            observation['temperature'] = {
+                'value': get_data(row, 1)
+            }
+            observation['windSpeed'] = {
+                'value': get_data(row, 2, float, 1 / 0.28)
+            }
+            observation['windDirection'] = {
+                'value': decode_wind_direction(row[3])
+            }
+            observation['precipitation'] = {
+                'value': get_data(row, 6)
+            }
+            observation['atmosphericPressure'] = {
+                'value': get_data(row, 7)
+            }
+            observation['pressureTendency'] = {
+                'value': get_data(row, 8)
+            }
+            observation['relativeHumidity'] = {
+                'value': get_data(row, 9, factor=100.0)
+            }
+
+            date_observed = datetime.datetime.strptime(
+                row[0], '%d/%m/%Y %H:%M')
+            observation['dateObserved'] = {
+                'value': date_observed.replace(tzinfo=madrid_tz).isoformat(),
+                'type': 'DateTime'
+            }
+            observation['source'] = {
+                'value': 'http://www.aemet.es',
+                'type': 'URL'
+            }
+            observation['dataProvider'] = {
+                'value': 'TEF'
+            }
+            observation['address'] = {
+                'value': {
+                    'addressLocality': sanitize(
+                        station_data[station_code]['address']),
+                    'addressCountry': 'ES'},
+                'type': 'PostalAddress'}
+            observation['location'] = station_data[station_code]['location']
+
+            observation['id'] = 'Spain-WeatherObserved' + '-' + \
+                station_code + '-' + date_observed.isoformat()
+
+            out.append(observation)
+
+        f.close()
+
+        # Last observation is tagged as 'latest'
+        if len(out) > 0:
+            latest_observation = out[-1]
+            latest_observation['id'] = 'Spain-WeatherObserved' + \
+                '-' + station_code + '-' + 'latest'
+
+        # A batch of station data is persisted
+        post_station_data_batch(station_code, out)
 
 
 # POST data to an Orion Context Broker instance using NGSIv2 API
 def post_station_data_batch(station_code, data):
-  data_obj = {
-    'actionType': 'APPEND',
-    'entities': data
-  }
-  data_as_str = json.dumps(data_obj)
-  
-  headers = {
-    'Content-Type':   MIME_JSON,
-    'Content-Length': len(data_as_str),
-    'Fiware-Service': FIWARE_SERVICE,
-    'Fiware-Servicepath': FIWARE_SPATH
-  }
-  
-  logger.debug('Going to persist %s (%d) to %s', station_code, len(data), orion_service)
-  
-  req = urllib2.Request(url=(orion_service + '/v2/op/update'), data=data_as_str, headers=headers)
-  
-  try:
-    with contextlib.closing(urllib2.urlopen(req)) as f:
-      global persisted_entities
-      global persisted_stations
-      persisted_entities = persisted_entities + len(data)
-      persisted_stations += 1
-      logger.debug('Entities successfully created for station: %s %d/%d',
-                   station_code, persisted_stations, total_stations)
-  except urllib2.URLError as e:
-    logger.error('Error!!! %s', station_code)
-    global in_error_entities
-    logger.error('Error while POSTing data to Orion: %d %s', e.code, e.read())
-    logger.debug('Data which failed: %s', data_as_str)
-    in_error_entities = in_error_entities + 1
-    
-    
-  
+    data_obj = {
+        'actionType': 'APPEND',
+        'entities': data
+    }
+    data_as_str = json.dumps(data_obj)
+
+    headers = {
+        'Content-Type': MIME_JSON,
+        'Content-Length': len(data_as_str),
+        'Fiware-Service': FIWARE_SERVICE,
+        'Fiware-Servicepath': FIWARE_SPATH
+    }
+
+    logger.debug(
+        'Going to persist %s (%d) to %s',
+        station_code,
+        len(data),
+        orion_service)
+
+    req = urllib2.Request(
+        url=(
+            orion_service +
+            '/v2/op/update'),
+        data=data_as_str,
+        headers=headers)
+
+    try:
+        with contextlib.closing(urllib2.urlopen(req)) as f:
+            global persisted_entities
+            global persisted_stations
+            persisted_entities = persisted_entities + len(data)
+            persisted_stations += 1
+            logger.debug('Entities successfully created for station: %s %d/%d',
+                         station_code, persisted_stations, total_stations)
+    except urllib2.URLError as e:
+        logger.error('Error!!! %s', station_code)
+        global in_error_entities
+        logger.error(
+            'Error while POSTing data to Orion: %d %s',
+            e.code,
+            e.read())
+        logger.debug('Data which failed: %s', data_as_str)
+        in_error_entities = in_error_entities + 1
+
+
 # Reads station data from CSV file
 def read_station_csv():
-  with contextlib.closing(open('../stations-normalized-wgs84.csv', 'rU')) as csvfile:
-    reader = csv.reader(csvfile, delimiter=',')
-    index = 0
-    for row in reader:
-      if index != 0:
-        station_code = row[2]
-        station_name = row[3]
-        station_address = row[4]
-        station_coords = {
-          'type': 'geo:json',
-          'value': {
-            'type': 'Point',
-            'coordinates': [float(row[0]), float(row[1])]
-          } 
-        }
-        
-        station_data[station_code] = {
-          'name': station_name,
-          'address': station_address,
-          'location': station_coords
-        }
-      index += 1
-    
-      
+    with contextlib.closing(open('../stations-normalized-wgs84.csv', 'rU')) as csvfile:
+        reader = csv.reader(csvfile, delimiter=',')
+        index = 0
+        for row in reader:
+            if index != 0:
+                station_code = row[2]
+                station_name = row[3]
+                station_address = row[4]
+                station_coords = {
+                    'type': 'geo:json',
+                    'value': {
+                        'type': 'Point',
+                        'coordinates': [float(row[0]), float(row[1])]
+                    }
+                }
+
+                station_data[station_code] = {
+                    'name': station_name,
+                    'address': station_address,
+                    'location': station_coords
+                }
+            index += 1
+
+
 def setup_logger():
-  global logger
-  
-  LOG_FILENAME = 'harvest_weather_observed_spain.log'
-
-  # Set up a specific logger with our desired output level
-  logger = logging.getLogger('WeatherObserved')
-  logger.setLevel(logging.DEBUG)
-
-  #  Add the log message handler to the logger
-  handler = logging.handlers.RotatingFileHandler(LOG_FILENAME, maxBytes=2000000, backupCount=3)
-  formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
-  handler.setFormatter(formatter)
-  
-  logger.addHandler(handler)
-  
-     
+    global logger
+
+    LOG_FILENAME = 'harvest_weather_observed_spain.log'
+
+    # Set up a specific logger with our desired output level
+    logger = logging.getLogger('WeatherObserved')
+    logger.setLevel(logging.DEBUG)
+
+    #  Add the log message handler to the logger
+    handler = logging.handlers.RotatingFileHandler(
+        LOG_FILENAME, maxBytes=2000000, backupCount=3)
+    formatter = logging.Formatter('%(levelname)s %(asctime)s %(message)s')
+    handler.setFormatter(formatter)
+
+    logger.addHandler(handler)
+
+
 if __name__ == '__main__':
-  setup_logger()
-  
-  read_station_csv()
-  
-  logger.debug('#### Starting a new harvesting and harmonization cycle ... ####') 
-  logger.debug('Number of weather stations known: %d', len(station_data.keys()))
-  total_stations = len(station_data.keys()) - len(station_code_exceptions)
-  
-  get_weather_observed_spain()
-  
-  logger.debug('Number of entities persisted: %d', persisted_entities)
-  logger.debug('Number of entities already existed: %d', already_existing_entities)
-  logger.debug('Number of stations in error: %d', in_error_entities)
-  logger.debug('#### Harvesting cycle finished ... ####')
-  
+    setup_logger()
+
+    read_station_csv()
+
+    logger.debug(
+        '#### Starting a new harvesting and harmonization cycle ... ####')
+    logger.debug(
+        'Number of weather stations known: %d', len(
+            station_data.keys()))
+    total_stations = len(station_data.keys()) - len(station_code_exceptions)
+
+    get_weather_observed_spain()
+
+    logger.debug('Number of entities persisted: %d', persisted_entities)
+    logger.debug(
+        'Number of entities already existed: %d',
+        already_existing_entities)
+    logger.debug('Number of stations in error: %d', in_error_entities)
+    logger.debug('#### Harvesting cycle finished ... ####')
diff --git a/Weather/WeatherObserved/harvest/weather_observed.py b/Weather/WeatherObserved/harvest/weather_observed.py
index 02e9806..0511464 100644
--- a/Weather/WeatherObserved/harvest/weather_observed.py
+++ b/Weather/WeatherObserved/harvest/weather_observed.py
@@ -11,79 +11,82 @@
 
 weather_observed = "http://www.aemet.es/es/eltiempo/observacion/ultimosdatos_{}_datos-horarios.csv?k=cle&l={}&datos=det&w=0&f=temperatura&x=h6"
 
+
 def get_data(row, index, conversion=float, factor=1.0):
-  out = None
-  
-  value = row[index]
-  if(value != ''):
-    out = conversion(value) / factor
-    
-  return out
-
-def get_weather_observed(request):    
+    out = None
+
+    value = row[index]
+    if(value != ''):
+        out = conversion(value) / factor
+
+    return out
+
+
+def get_weather_observed(request):
     query = request.args.get('q')
-    
+
     if not query:
-      return Response(json.dumps([]), mimetype='application/json')
-    
-    tokens  = query.split(';')
-    
+        return Response(json.dumps([]), mimetype='application/json')
+
+    tokens = query.split(';')
+
     station_code = ''
     country = ''
-    
+
     for token in tokens:
-      items = token.split(':')
-      if items[0] == 'stationCode':
-        station_code = items[1]
-      elif items[0] == 'country':
-        country = items[1]
-    
+        items = token.split(':')
+        if items[0] == 'stationCode':
+            station_code = items[1]
+        elif items[0] == 'country':
+            country = items[1]
+
     if not station_code or not country or country != 'ES':
-      return Response(json.dumps([]), mimetype='application/json')
-    
+        return Response(json.dumps([]), mimetype='application/json')
+
     source = weather_observed.format(station_code, station_code)
-    
+
     req = urllib2.Request(url=source)
     f = urllib2.urlopen(req)
     csv_data = f.read()
-    
+
     csv_file = StringIO.StringIO(csv_data)
     reader = csv.reader(csv_file, delimiter=',')
-    
+
     out = []
     index = 0
     for row in reader:
-      if index == 0:
-        address = row[0]
-        
-      if index < 4:
-        index += 1
-        continue
-      
-      print(row)
-      
-      observation = {
-        'type': 'WeatherObserved'
-      }
-      if len(row) < 2:
-        continue
-      
-      observation['temperature'] = get_data(row, 1)
-      observation['windSpeed'] = get_data(row, 2, int)
-      observation['windDirection'] = row[3] or None
-      observation['precipitation'] = get_data(row, 6)
-      observation['pressure'] = get_data(row, 7)
-      observation['pressureTendency'] =  get_data(row, 8)
-      observation['relativeHumidity'] = get_data(row, 9, factor=100.0)
-      
-      observation['dateObserved'] = datetime.datetime.strptime(row[0], '%d/%m/%Y %H:%M').isoformat()
-      observation['source'] = 'http://www.aemet.es'
-      observation['address'] = {
-        'addressLocality': address.decode('latin-1'),
-        'addressCountry': country
-      }
-      
-      out.append(observation)
-    
+        if index == 0:
+            address = row[0]
+
+        if index < 4:
+            index += 1
+            continue
+
+        print(row)
+
+        observation = {
+            'type': 'WeatherObserved'
+        }
+        if len(row) < 2:
+            continue
+
+        observation['temperature'] = get_data(row, 1)
+        observation['windSpeed'] = get_data(row, 2, int)
+        observation['windDirection'] = row[3] or None
+        observation['precipitation'] = get_data(row, 6)
+        observation['pressure'] = get_data(row, 7)
+        observation['pressureTendency'] = get_data(row, 8)
+        observation['relativeHumidity'] = get_data(row, 9, factor=100.0)
+
+        observation['dateObserved'] = datetime.datetime.strptime(
+            row[0], '%d/%m/%Y %H:%M').isoformat()
+        observation['source'] = 'http://www.aemet.es'
+        observation['address'] = {
+            'addressLocality': address.decode('latin-1'),
+            'addressCountry': country
+        }
+
+        out.append(observation)
+
     print(out)
-    return Response(json.dumps(out), mimetype='application/json')
\ No newline at end of file
+    return Response(json.dumps(out), mimetype='application/json')
diff --git a/Weather/aemet.py b/Weather/aemet.py
index 472dc66..481fb74 100644
--- a/Weather/aemet.py
+++ b/Weather/aemet.py
@@ -17,243 +17,262 @@
 from weather_observed import get_weather_observed
 
 postal_codes = {
-  '47001': '47186',
-  '28001': '28079',
-  '39001': '39075',
-  '34001': '34120',
-  '34200': '34023',
-  '05194': '05123',
-  '33300': '33076',
-  '41001': '41091'
+    '47001': '47186',
+    '28001': '28079',
+    '39001': '39075',
+    '34001': '34120',
+    '34200': '34023',
+    '05194': '05123',
+    '33300': '33076',
+    '41001': '41091'
 }
 
 localities = {
-  'Valladolid':         '47186',
-  'Madrid':             '28079',
-  'Santander':          '39075',
-  'Palencia':           '34120',
-  u'Venta de Ba帽os':    '34023',
-  'Mediana de Voltoya': '05123',
-  'Villaviciosa':       '33076',
-  'Sevilla':            '41091'
+    'Valladolid': '47186',
+    'Madrid': '28079',
+    'Santander': '39075',
+    'Palencia': '34120',
+    u'Venta de Ba帽os': '34023',
+    'Mediana de Voltoya': '05123',
+    'Villaviciosa': '33076',
+    'Sevilla': '41091'
 }
 
 app = Flask(__name__)
 
-aemet_service    = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+aemet_service = "http://www.aemet.es/xml/municipios/localidad_{}.xml"
+
 
 @app.route('/')
 def index():
     return "Hello, World!"
 
-  
-@app.route('/v2/entities',  methods=['GET'])
+
+@app.route('/v2/entities', methods=['GET'])
 def get_weather():
     entity_type = request.args.get('type')
-      
+
     if entity_type == 'WeatherForecast':
-      return get_weather_forecasted(request)
+        return get_weather_forecasted(request)
     elif entity_type == 'WeatherObserved':
-      return get_weather_observed(request)
+        return get_weather_observed(request)
     elif entity_type == 'WeatherAlarm':
-      return get_weather_alarms(request)
+        return get_weather_alarms(request)
     else:
-      return Response(json.dumps([]), mimetype='application/json')
-    
-  
+        return Response(json.dumps([]), mimetype='application/json')
+
+
 def get_data(row, index, conversion=float, factor=1.0):
-  out = None
-  
-  value = row[index]
-  if(value != ''):
-    out = conversion(value) / factor
-    
-  return out
-    
-def get_weather_forecasted(request):    
+    out = None
+
+    value = row[index]
+    if(value != ''):
+        out = conversion(value) / factor
+
+    return out
+
+
+def get_weather_forecasted(request):
     country = ''
     postal_code = ''
     address_locality = ''
-    
+
     query = request.args.get('q')
-    
+
     if not query:
-      return Response(json.dumps([]), mimetype='application/json')
-    
-    tokens  = query.split(';')
+        return Response(json.dumps([]), mimetype='application/json')
+
+    tokens = query.split(';')
     for token in tokens:
-      items = token.split(':')
-      if items[0] == 'postalCode':
-        postal_code = items[1]
-      elif items[0] == 'country':
-        country = items[1]
-      elif items[0] == 'addressLocality':
-        address_locality = items[1]
-        
+        items = token.split(':')
+        if items[0] == 'postalCode':
+            postal_code = items[1]
+        elif items[0] == 'country':
+            country = items[1]
+        elif items[0] == 'addressLocality':
+            address_locality = items[1]
+
     if country == 'PT' and address_locality:
-      return Response(json.dumps(get_weather_forecasted_pt(address_locality)), mimetype='application/json')
-    
-    if not country or (not postal_code in postal_codes and not address_locality in localities) or country != 'ES':
-      return Response(json.dumps([]), mimetype='application/json')
-    
+        return Response(
+            json.dumps(
+                get_weather_forecasted_pt(address_locality)),
+            mimetype='application/json')
+
+    if not country or (
+            postal_code in postal_codes and not address_locality not in localities) or country != 'ES':
+        return Response(json.dumps([]), mimetype='application/json')
+
     param = ''
     if postal_code:
-      param = postal_codes[postal_code]
+        param = postal_codes[postal_code]
     elif address_locality:
-      param = localities[address_locality]
-    
+        param = localities[address_locality]
+
     source = aemet_service.format(param)
     req = urllib2.Request(url=source)
     f = urllib2.urlopen(req)
     xml_data = f.read()
     DOMTree = xml.dom.minidom.parseString(xml_data).documentElement
-    
-    address_locality = DOMTree.getElementsByTagName('nombre')[0].firstChild.nodeValue
-    address = { }
+
+    address_locality = DOMTree.getElementsByTagName(
+        'nombre')[0].firstChild.nodeValue
+    address = {}
     address['addressCountry'] = country
     address['postalCode'] = postal_code
     address['addressLocality'] = address_locality
-    
-    created =  DOMTree.getElementsByTagName('elaborado')[0].firstChild.nodeValue
-    
-    forecasts = DOMTree.getElementsByTagName('prediccion')[0].getElementsByTagName('dia')
-    
+
+    created = DOMTree.getElementsByTagName('elaborado')[0].firstChild.nodeValue
+
+    forecasts = DOMTree.getElementsByTagName(
+        'prediccion')[0].getElementsByTagName('dia')
+
     out = []
     for forecast in forecasts:
-      date = forecast.getAttribute('fecha')
-      normalizedForecast = parse_aemet_forecast(forecast, date)
-      counter = 1
-      for f in normalizedForecast:
-        f['type'] = 'WeatherForecast'
-        f['id'] = generate_id(postal_code, country, date) + '_' + str(counter)
-        f['address'] = address
-        f['dateCreated'] = created
-        f['source'] = source
-        counter+=1
-        out.append(f)
-    
+        date = forecast.getAttribute('fecha')
+        normalizedForecast = parse_aemet_forecast(forecast, date)
+        counter = 1
+        for f in normalizedForecast:
+            f['type'] = 'WeatherForecast'
+            f['id'] = generate_id(
+                postal_code, country, date) + '_' + str(counter)
+            f['address'] = address
+            f['dateCreated'] = created
+            f['source'] = source
+            counter += 1
+            out.append(f)
+
     return Response(json.dumps(out), mimetype='application/json')
-    
+
 
 def parse_aemet_forecast(forecast, date):
-  periods = { }
-  out = []
-  
-  parsed_date = parser.parse(date)
-  
-  pops = forecast.getElementsByTagName('prob_precipitacion')
-  for pop in pops:
-    period = pop.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if pop.firstChild and pop.firstChild.nodeValue:
-      insert_into_period(periods, period,
-                  'precipitationProbability', float(pop.firstChild.nodeValue) / 100.0)
-  
-  period = None
-  weather_types = forecast.getElementsByTagName('estado_cielo')
-  for weather_type in weather_types:
-    period = weather_type.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    if weather_type.firstChild and weather_type.firstChild.nodeValue:
-      insert_into_period(periods, period, 'weatherType',
-                         weather_type.getAttribute('descripcion'))
-  
-  period = None
-  wind_data = forecast.getElementsByTagName('viento')
-  for wind in wind_data:
-    period = wind.getAttribute('periodo')
-    if not period:
-      period = '00-24'
-    wind_direction = wind.getElementsByTagName('direccion')[0]
-    wind_speed = wind.getElementsByTagName('velocidad')[0]
-    if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windSpeed',
-                         int(wind_speed.firstChild.nodeValue))
-    if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
-      insert_into_period(periods, period, 'windDirection',
-                         wind_direction.firstChild.nodeValue)
-  
-  temperature_node = forecast.getElementsByTagName('temperatura')[0]
-  max_temp = float(temperature_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp = float(temperature_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temperature_node, periods, 'temperature')
-  
-  temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
-  max_temp_feels = float(temp_feels_node.getElementsByTagName('maxima')[0].firstChild.nodeValue)
-  min_temp_feels = float(temp_feels_node.getElementsByTagName('minima')[0].firstChild.nodeValue)
-  get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
-  
-  humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
-  max_humidity = float(humidity_node.getElementsByTagName('maxima')[0].firstChild.nodeValue) / 100.0
-  min_humidity = float(humidity_node.getElementsByTagName('minima')[0].firstChild.nodeValue) / 100.0
-  get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
-  
-  for period in periods:
-    print(period)
-  
-  for period in periods:
-    period_items = period.split('-')
-    period_start = period_items[0]
-    period_end = period_items[1]
-    end_hour = int(period_end)
-    end_date = copy.deepcopy(parsed_date)
-    if end_hour > 23:
-      end_hour = 0
-      end_date = parsed_date + datetime.timedelta(days=1)
-    
-    start_date = parsed_date.replace(hour=int(period_start), minute=0, second=0)
-    end_date = end_date.replace(hour=end_hour,minute=0,second=0)   
-    
-    objPeriod = periods[period]
-    objPeriod['validity'] = { }
-    objPeriod['validity']['from'] = start_date.isoformat()
-    objPeriod['validity']['to'] = end_date.isoformat()
-    
-    maximum = { }
-    objPeriod['dayMaximum'] = maximum
-    minimum = { }
-    objPeriod['dayMinimum'] = minimum
-    
-    maximum['temperature'] = max_temp
-    minimum['temperature'] = min_temp
-    
-    maximum['relativeHumidity'] = max_humidity
-    minimum['relativeHumidity'] = min_humidity
-    
-    maximum['feelsLikeTemperature'] = max_temp_feels
-    minimum['feelsLikeTemperature'] = min_temp_feels
-    
-    out.append(objPeriod)
-  
-  return out
+    periods = {}
+    out = []
+
+    parsed_date = parser.parse(date)
+
+    pops = forecast.getElementsByTagName('prob_precipitacion')
+    for pop in pops:
+        period = pop.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if pop.firstChild and pop.firstChild.nodeValue:
+            insert_into_period(
+                periods, period, 'precipitationProbability', float(
+                    pop.firstChild.nodeValue) / 100.0)
+
+    period = None
+    weather_types = forecast.getElementsByTagName('estado_cielo')
+    for weather_type in weather_types:
+        period = weather_type.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        if weather_type.firstChild and weather_type.firstChild.nodeValue:
+            insert_into_period(periods, period, 'weatherType',
+                               weather_type.getAttribute('descripcion'))
+
+    period = None
+    wind_data = forecast.getElementsByTagName('viento')
+    for wind in wind_data:
+        period = wind.getAttribute('periodo')
+        if not period:
+            period = '00-24'
+        wind_direction = wind.getElementsByTagName('direccion')[0]
+        wind_speed = wind.getElementsByTagName('velocidad')[0]
+        if wind_speed.firstChild and wind_speed.firstChild.nodeValue:
+            insert_into_period(periods, period, 'windSpeed',
+                               int(wind_speed.firstChild.nodeValue))
+        if wind_direction.firstChild and wind_direction.firstChild.nodeValue:
+            insert_into_period(periods, period, 'windDirection',
+                               wind_direction.firstChild.nodeValue)
+
+    temperature_node = forecast.getElementsByTagName('temperatura')[0]
+    max_temp = float(temperature_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp = float(temperature_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temperature_node, periods, 'temperature')
+
+    temp_feels_node = forecast.getElementsByTagName('sens_termica')[0]
+    max_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue)
+    min_temp_feels = float(temp_feels_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue)
+    get_parameter_data(temp_feels_node, periods, 'feelsLikeTemperature')
+
+    humidity_node = forecast.getElementsByTagName('humedad_relativa')[0]
+    max_humidity = float(humidity_node.getElementsByTagName(
+        'maxima')[0].firstChild.nodeValue) / 100.0
+    min_humidity = float(humidity_node.getElementsByTagName(
+        'minima')[0].firstChild.nodeValue) / 100.0
+    get_parameter_data(humidity_node, periods, 'relativeHumidity', 100.0)
+
+    for period in periods:
+        print(period)
+
+    for period in periods:
+        period_items = period.split('-')
+        period_start = period_items[0]
+        period_end = period_items[1]
+        end_hour = int(period_end)
+        end_date = copy.deepcopy(parsed_date)
+        if end_hour > 23:
+            end_hour = 0
+            end_date = parsed_date + datetime.timedelta(days=1)
+
+        start_date = parsed_date.replace(
+            hour=int(period_start), minute=0, second=0)
+        end_date = end_date.replace(hour=end_hour, minute=0, second=0)
+
+        objPeriod = periods[period]
+        objPeriod['validity'] = {}
+        objPeriod['validity']['from'] = start_date.isoformat()
+        objPeriod['validity']['to'] = end_date.isoformat()
+
+        maximum = {}
+        objPeriod['dayMaximum'] = maximum
+        minimum = {}
+        objPeriod['dayMinimum'] = minimum
+
+        maximum['temperature'] = max_temp
+        minimum['temperature'] = min_temp
+
+        maximum['relativeHumidity'] = max_humidity
+        minimum['relativeHumidity'] = min_humidity
+
+        maximum['feelsLikeTemperature'] = max_temp_feels
+        minimum['feelsLikeTemperature'] = min_temp_feels
+
+        out.append(objPeriod)
+
+    return out
 
 
 def get_parameter_data(node, periods, parameter, factor=1.0):
-  param_periods = node.getElementsByTagName('dato')
-  for param in param_periods:
-    hour_str = param.getAttribute('hora')
-    hour = int(hour_str)
-    interval_start = hour - 6
-    interval_start_str = str(interval_start)
-    if interval_start < 10:
-      interval_start_str = '0' + str(interval_start)
-      
-    period = interval_start_str + '-' + hour_str
-    if param.firstChild and param.firstChild.nodeValue:
-      param_val = float(param.firstChild.nodeValue)
-      insert_into_period(periods, period, parameter, param_val / factor)
+    param_periods = node.getElementsByTagName('dato')
+    for param in param_periods:
+        hour_str = param.getAttribute('hora')
+        hour = int(hour_str)
+        interval_start = hour - 6
+        interval_start_str = str(interval_start)
+        if interval_start < 10:
+            interval_start_str = '0' + str(interval_start)
+
+        period = interval_start_str + '-' + hour_str
+        if param.firstChild and param.firstChild.nodeValue:
+            param_val = float(param.firstChild.nodeValue)
+            insert_into_period(periods, period, parameter, param_val / factor)
 
 
 def insert_into_period(periods, period, attribute, value):
-  if not period in periods:
-    periods[period] = { }
-  
-  periods[period][attribute] = value
+    if period not in periods:
+        periods[period] = {}
+
+    periods[period][attribute] = value
+
 
 def generate_id(postal_code, country, date):
-  return postal_code + '_' + country + '_' + date
+    return postal_code + '_' + country + '_' + date
+
 
 if __name__ == '__main__':
-    app.run(host='0.0.0.0',port=1028,debug=True)
+    app.run(host='0.0.0.0', port=1028, debug=True)

From f296bf4b90159b5df3dc6a34fe2e377e67fba21d Mon Sep 17 00:00:00 2001
From: cclauss <cclauss@bluewin.ch>
Date: Tue, 3 Oct 2017 17:30:22 +0200
Subject: [PATCH 2/2] Fix long line 1 of 2

---
 Weather/WeatherObserved/harvest/weather_observed.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/Weather/WeatherObserved/harvest/weather_observed.py b/Weather/WeatherObserved/harvest/weather_observed.py
index 0511464..fc59699 100644
--- a/Weather/WeatherObserved/harvest/weather_observed.py
+++ b/Weather/WeatherObserved/harvest/weather_observed.py
@@ -9,7 +9,8 @@
 import datetime
 import json
 
-weather_observed = "http://www.aemet.es/es/eltiempo/observacion/ultimosdatos_{}_datos-horarios.csv?k=cle&l={}&datos=det&w=0&f=temperatura&x=h6"
+weather_observed = ("http://www.aemet.es/es/eltiempo/observacion/ultimosdatos_{}_datos-horarios.csv"
+                    "?k=cle&l={}&datos=det&w=0&f=temperatura&x=h6")
 
 
 def get_data(row, index, conversion=float, factor=1.0):
